conf_abbr,conference_name,year,name,author_info,article_link,abstract
cvpr,CVPR (Computer Vision),2019,A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction,"Shumian Xin, Carnegie Mellon University|; et al.|Sotiris Nousias, University College London|Kiriakos N. Kutulakos, University of Toronto|Aswin C. Sankaranarayanan, Carnegie Mellon University|Srinivasa G. Narasimhan, Carnegie Mellon University|Ioannis Gkioulekas, Carnegie Mellon University",semanticscholar.org/paper/5809decf0a47a8a6be5c0525cd9f92e1e7d4c75f,"We present a novel theory of Fermat paths of light between a known visible scene and an unknown object not in the line of sight of a transient camera. These light paths either obey specular reflection or are reflected by the object's boundary, and hence encode the shape of the hidden object. We prove that Fermat paths correspond to discontinuities in the transient measurements. We then derive a novel constraint that relates the spatial derivatives of the path lengths at these discontinuities to the surface normal. Based on this theory, we present an algorithm, called Fermat Flow, to estimate the shape of the non-line-of-sight object. Our method allows, for the first time, accurate shape recovery of complex objects, ranging from diffuse to specular, that are hidden around the corner as well as hidden behind a diffuser. Finally, our approach is agnostic to the particular technology used for transient imaging. As such, we demonstrate mm-scale shape recovery from pico-second scale transients using a SPAD and ultrafast laser, as well as micron-scale reconstruction from femto-second scale transients using interferometry. We believe our work is a significant advance over the state-of-the-art in non-line-of-sight imaging."
cvpr,CVPR (Computer Vision),2018,Taskonomy: Disentangling Task Transfer Learning,"Amir R. Zamir, Stanford University|; et al.|Alexander Sax, Stanford University|William Shen, Stanford University|Leonidas Guibas, Stanford University|Jitendra Malik, University of California, Berkeley|Silvio Savarese, Stanford University",https://www.semanticscholar.org/paper/2fe2cfd98e232f1396f01881853ed6b3d5e37d65,"Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity. We proposes a fully computational approach for modeling the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. We provide a set of tools for computing and probing this taxonomical structure including a solver users can employ to find supervision policies for their use cases."
cvpr,CVPR (Computer Vision),2017,Densely Connected Convolutional Networks,"Zhuang Liu, Tsinghua University|; et al.|Gao Huang, Cornell University|Laurens van der Maaten, Facebook AI Research|Kilian Q. Weinberger, Cornell University",https://www.semanticscholar.org/paper/5694e46284460a648fe29117cbc55f6c9be3fa3c,"Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections&#x2014;one between each layer and its subsequent layer&#x2014;our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet."
cvpr,CVPR (Computer Vision),2017,Learning from Simulated and Unsupervised Images through Adversarial Training,"Ashish Shrivastava, Apple Inc.|; et al.|Tomas Pfister, Apple Inc.|Oncel Tuzel, Apple Inc.|Josh Susskind, Apple Inc.|Wenda Wang, Apple Inc.|Russ Webb, Apple Inc.",https://www.semanticscholar.org/paper/68cb9fce1e6af2740377494350b650533c9a29e1,"With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data."
cvpr,CVPR (Computer Vision),2016,Deep Residual Learning for Image Recognition,"Kaiming He, Microsoft Research|; et al.|Xiangyu Zhang, Microsoft Research|Shaoqing Ren, Microsoft Research|Jian Sun, Microsoft Research",https://www.semanticscholar.org/paper/29c808b346526fbb6027e67942b62a40a549f019,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
cvpr,CVPR (Computer Vision),2015,DynamicFusion: Reconstruction and Tracking of Non-rigid Scenes in Real-Time,"Richard A. Newcombe, University of Washington|; et al.|Dieter Fox, University of Washington|Steven M. Seitz, University of Washington",https://www.semanticscholar.org/paper/e37f2fb8d6e675abbcda3bd09d586b9aaec26486,"We present the first dense SLAM system capable of reconstructing non-rigidly deforming scenes in real-time, by fusing together RGBD scans captured from commodity sensors. Our DynamicFusion approach reconstructs scene geometry whilst simultaneously estimating a dense volumetric 6D motion field that warps the estimated geometry into a live frame. Like KinectFusion, our system produces increasingly denoised, detailed, and complete reconstructions as more measurements are fused, and displays the updated model in real time. Because we do not require a template or other prior scene model, the approach is applicable to a wide range of moving objects and scenes."
cvpr,CVPR (Computer Vision),2014,What Camera Motion Reveals About Shape with Unknown BRDF,"Manmohan Chandraker, NEC Labs America",https://www.semanticscholar.org/paper/230d5bbbce1e5538941393a33154a9aee9b89ca7,"Psychophysical studies show motion cues inform about shape even with unknown reflectance. Recent works in computer vision have considered shape recovery for an object of unknown BRDF using light source or object motions. This paper addresses the remaining problem of determining shape from the (small or differential) motion of the camera, for unknown isotropic BRDFs. Our theory derives a differential stereo relation that relates camera motion to depth of a surface with unknown isotropic BRDF, which generalizes traditional Lambertian assumptions. Under orthographic projection, we show shape may not be constrained in general, but two motions suffice to yield an invariant for several restricted (still unknown) BRDFs exhibited by common materials. For the perspective case, we show that three differential motions suffice to yield surface depth for unknown isotropic BRDF and unknown directional lighting, while additional constraints are obtained with restrictions on BRDF or lighting. The limits imposed by our theory are intrinsic to the shape recovery problem and independent of choice of reconstruction method. We outline with experiments how potential reconstruction methods may exploit our theory. We illustrate trends shared by theories on shape from motion of light, object or camera, relating reconstruction hardness to imaging complexity."
cvpr,CVPR (Computer Vision),2013,"Fast, Accurate Detection of 100,000 Object Classes on a Single Machine","Thomas Dean, Google|; et al.|Mark A. Ruzon, Google|Mark Segal, Google|Jonathon Shlens, Google|Sudheendra Vijayanarasimhan, Google|Jay Yagnik, Google",https://www.semanticscholar.org/paper/774f67303ea4a3a94874f08cf9a9dacc69b40782,"Many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object's appearance, such as the presence of component parts. We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank. To show the effectiveness of the technique, we apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM. This represents a speed-up of approximately 20,000 times - four orders of magnitude - when compared with performing the convolutions explicitly on the same hardware. While mean average precision over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes."
cvpr,CVPR (Computer Vision),2012,A Simple Prior-free Method for Non-Rigid Structure-from-Motion Factorization,"Yuchao Dai, Northwestern Polytechnical University|; et al.|Hongdong Li, Australian National University|Mingyi He, Northwestern Polytechnical University",https://www.semanticscholar.org/paper/c4eb1bbcc2c07db85f04af3572be8493bcea9d00,"This paper proposes a simple “prior-free” method for solving non-rigid structure-from-motion factorization problems. Other than using the basic low-rank condition, our method does not assume any extra prior knowledge about the nonrigid scene or about the camera motions. Yet, it runs reliably, produces optimal result, and does not suffer from the inherent basis-ambiguity issue which plagued many conventional nonrigid factorization techniques. Our method is easy to implement, which involves solving no more than an SDP (semi-definite programming) of small and fixed size, a linear Least-Squares or trace-norm minimization. Extensive experiments have demonstrated that it outperforms most of the existing linear methods of nonrigid factorization. This paper offers not only new theoretical insight, but also a practical, everyday solution, to non-rigid structure-from-motion."
cvpr,CVPR (Computer Vision),2011,Real-time Human Pose Recognition in Parts from Single Depth Images,"Jamie Shotton, Microsoft Research|; et al.|Andrew Fitzgibbon, Microsoft Research|Mat Cook, Microsoft Research|Toby Sharp, Microsoft Research|Mark Finocchio, Microsoft Research|Richard Moore, Microsoft Research|Alex Kipman, Microsoft Research|Andrew Blake, Microsoft Research",https://www.semanticscholar.org/paper/ee3157ce19551b62012ab7a27c164d6f30242076,"Semantic Scholar extracted view of ""Real-Time Human Pose Recognition in Parts from Single Depth Images"" by J. Shotton et al."
cvpr,CVPR (Computer Vision),2009,Single Image Haze Removal Using Dark Channel Prior,"Kaiming He, The Chinese University of Hong Kong|; et al.|Jian Sun, Microsoft Research|Xiaoou Tang, The Chinese University of Hong Kong",https://www.semanticscholar.org/paper/db0fd1d8b1ef0bf19a137cd535166ed067455280,"In this paper, we propose a simple but effective image prior—dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of outdoor haze-free images...."
cvpr,CVPR (Computer Vision),2008,Global Stereo Reconstruction under Second Order Smoothness Priors,"Oliver Woodford, University of Oxford|; et al.|Ian Reid, Oxford Brookes University|Philip Torr, University of Oxford|Andrew Fitzgibbon, Microsoft Research",https://www.semanticscholar.org/paper/8e7f51853229843c0056681f97754869032348ee,"Second-order priors on the smoothness of 3D surfaces are a better model of typical scenes than first-order priors. However, stereo reconstruction using global inference algorithms, such as graph-cuts, has not been able to incorporate second-order priors because the triple cliques needed to express them yield intractable (non-submodular) optimization problems. This paper shows that inference with triple cliques can be effectively optimized. Our optimization strategy is a development of recent extensions to a-expansion, based on the ""QPBO"" algorithm [5, 14, 26]. The strategy is to repeatedly merge proposal depth maps using a novel extension of QPBO. Proposal depth maps can come from any source, for example fronto-parallel planes as in a-expansion, or indeed any existing stereo algorithm, with arbitrary parameter settings. Experimental results demonstrate the usefulness of the second-order prior and the efficacy of our optimization framework. An implementation of our stereo framework is available online [34]."
cvpr,CVPR (Computer Vision),2008,Beyond Sliding Windows: Object Localization by Efficient Subwindow Search,"Chistoph H. Lampert, Max-Planck-Institut für Limnologie|; et al.|Matthew B. Blaschko, Max-Planck-Institut für Limnologie|Thomas Hodmann, Google",https://www.semanticscholar.org/paper/54b224478a63e33441c651175c522f3702062fc4,"Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To perform localization, one can take a sliding window approach, but this strongly increases the computational cost, because the classifier function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch-and-bound scheme that allows efficient maximization of a large class of classifier functions over all possible subimages. It converges to a globally optimal solution typically in sublinear time. We show how our method is applicable to different object detection and retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest neighbor classifiers based on the chi2-distance. We demonstrate state-of-the-art performance of the resulting systems on the UIUC Cars dataset, the PASCAL VOC 2006 dataset and in the PASCAL VOC 2007 competition."
cvpr,CVPR (Computer Vision),2007,Dynamic 3D Scene Analysis from a Moving Vehicle,"Bastian Leibe, ETH Zurich|; et al.|Nico Cornelis, Katholieke Universiteit Leuven|Kurt COrnelis, Katholieke Universiteit Leuven|Luc Van Gool, ETH Zurich",https://www.semanticscholar.org/paper/82fa8d73891ce6476d58f11d6e3b563af21d0a3a,"In this paper, we present a system that integrates fully automatic scene geometry estimation, 2D object detection, 3D localization, trajectory estimation, and tracking for dynamic scene interpretation from a moving vehicle. Our sole input are two video streams from a calibrated stereo rig on top of a car. From these streams, we estimate structure-from-motion (SfM) and scene geometry in real-time. In parallel, we perform multi-view/multi-category object recognition to detect cars and pedestrians in both camera images. Using the SfM self-localization, 2D object detections are converted to 3D observations, which are accumulated in a world coordinate frame. A subsequent tracking module analyzes the resulting 3D observations to find physically plausible spacetime trajectories. Finally, a global optimization criterion takes object-object interactions into account to arrive at accurate 3D localization and trajectory estimates for both cars and pedestrians. We demonstrate the performance of our integrated system on challenging real-world data showing car passages through crowded city areas."
cvpr,CVPR (Computer Vision),2006,Putting Objects in Perspective,"Derek Hoiem, Carnegie Mellon University|; et al.|Alexei Efros, Carnegie Mellon University|Martial Hebert, Carnegie Mellon University",https://www.semanticscholar.org/paper/baddac96864c86538d3bd8bf495f00f818475a9e,"Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach."
cvpr,CVPR (Computer Vision),2005,Real-Time Non-Rigid Surface Detection,"Julien Pilet, École Polytechnique Fédérale de Lausanne|; et al.|Vincent Lepetit, École Polytechnique Fédérale de Lausanne|Pascal Fua, École Polytechnique Fédérale de Lausanne",https://www.semanticscholar.org/paper/9197cc4555826c354722a02b784d24817889d445,"We present a real-time method for detecting deformable surfaces, with no need whatsoever for a priori pose knowledge. Our method starts from a set of wide baseline point matches between an undeformed image of the object and the image in which it is to be detected. The matches are used not only to detect but also to compute a precise mapping from one to the other. The algorithm is robust to large deformations, lighting changes, motion blur, and occlusions. It runs at 10 frames per second on a 2.8 GHz PC and we are not aware of any other published technique that produces similar results. Combining deformable meshes with a well designed robust estimator is key to dealing with the large number of parameters involved in modeling deformable surfaces and rejecting erroneous matches for error rates of up to 95%, which is considerably more than what is required in practice."
cvpr,CVPR (Computer Vision),2004,Programmable Imaging using a Digital Micromirror Array,"Shree K. Nayar, Columbia University|; et al.|Vlad Branzoi, Columbia University|Terry E. Boult, University of Colorado Boulder",https://www.semanticscholar.org/paper/5ca3bb0b4286923efc9bc59a84e3de7225b9c8d3,"We introduce the notion of a programmable imaging system. Such an imaging system provides a human user or a vision system significant control over the radiometric and geometric characteristics of the system. This flexibility is achieved using a programmable array of micro-mirrors. The orientations of the mirrors of the array can be controlled with high precision over space and time. This enables the system to select and modulate rays from the light field based on the needs of the application at hand. We have implemented a programmable imaging system that uses a digital micro-mirror device (DMD), which is used in digital light processing. Although the mirrors of this device can only be positioned in one of two states, we show that our system can be used to implement a wide variety of imaging functions, including, high dynamic range imaging, feature detection, and object recognition. We conclude with a discussion on how a micro-mirror array can be used to efficiently control field of view without the use of moving parts."
cvpr,CVPR (Computer Vision),2003,Object Class Recognition by Unsupervised Scale-Invariant Learning,"Rob Fergus, University of Oxford|; et al.|Pietro Perona, California Institute of Technology|Andrew Zisserman, University of Oxford",https://www.semanticscholar.org/paper/62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5,"We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals)."
cvpr,CVPR (Computer Vision),2001,Morphable 3D models from video,"Matthew Brand, Mitsubishi Electric Research Laboratories",https://www.semanticscholar.org/paper/b9d8f47ebf3fd68ee6d0e0858c68f1df0c81b47a,"Nonrigid 3D structure-from-motion and 2D optical flow can both be formulated as tensor factorization problems. The two problems can be made equivalent through a noisy affine transform, yielding a combined nonrigid structure-from-intensities problem that we solve via structured matrix decompositions. Often the preconditions for this factorization are violated by image noise and deficiencies of the data visa-vis the sample complexity of the problem. Both issues are remediated with careful use of rank constraints, norm constraints, and integration over uncertainty in the intensity values, yielding novel solutions for SVD under uncertainty, factorization under uncertainty, nonrigid factorization, and subspace optical flow. The resulting integrated algorithm can track and reconstruct in 3D nonrigid surfaces having very little texture, for example the smooth parts of the face. Working with low-resolution low-texture ""found video,"" these methods produce good tracking and 3D reconstruction results where prior algorithms fail."
cvpr,CVPR (Computer Vision),2000,Real-Time Tracking of Non-Rigid Objects using Mean Shift,"Dorin Comaniciu, Siemens Corporate Research|; et al.|Visvanathan Ramesh, Siemens Corporate Research|Peter Meer, Rutgers University",https://www.semanticscholar.org/paper/2cfa006b33084abe8160b001f9a24944cda25d05,"A new method for real time tracking of non-rigid objects seen from a moving camera is proposed. The central computational module is based on the mean shift iterations and finds the most probable target position in the current frame. The dissimilarity between the target model (its color distribution) and the target candidates is expressed by a metric derived from the Bhattacharyya coefficient. The theoretical analysis of the approach shows that it relates to the Bayesian framework while providing a practical, fast and efficient solution. The capability of the tracker to handle in real time partial occlusions, significant clutter, and target scale variations, is demonstrated for several image sequences."
icml,ICML (Machine Learning),2019,Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations,"Francesco Locatello, ETH Zurich|; et al.|Stefan Bauer, Max Planck Institute|Mario Lucic, Google Research|Gunnar Rätsch, ETH Zurich|Sylvain Gelly, Google Research|Bernhard Schölkopf, Max Planck Institute|Olivier Bachem, Google Research",https://www.semanticscholar.org/paper/9c5c794094fbf5da8c48df5c3242615dc0b1d245,"The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets."
icml,ICML (Machine Learning),2019,Rates of Convergence for Sparse Variational Gaussian Process Regression,"David R. Burt, University of Cambridge|; et al.|Carl E. Rasmussen, University of Cambridge|Mark van der Wilk, PROWLER.io",https://www.semanticscholar.org/paper/ef78947a3c17bb8428080e35776275dce97139f0,"Excellent variational approximations to Gaussian process posteriors have been developed which avoid the $\mathcal{O}\left(N^3\right)$ scaling with dataset size $N$. They reduce the computational cost to $\mathcal{O}\left(NM^2\right)$, with $M\ll N$ being the number of inducing variables, which summarise the process. While the computational cost seems to be linear in $N$, the true complexity of the algorithm depends on how $M$ must increase to ensure a certain quality of approximation. We address this by characterising the behavior of an upper bound on the KL divergence to the posterior. We show that with high probability the KL divergence can be made arbitrarily small by growing $M$ more slowly than $N$. A particular case of interest is that for regression with normally distributed inputs in D-dimensions with the popular Squared Exponential kernel, $M=\mathcal{O}(\log^D N)$ is sufficient. Our results show that as datasets grow, Gaussian process posteriors can truly be approximated cheaply, and provide a concrete rule for how to increase $M$ in continual learning scenarios."
icml,ICML (Machine Learning),2018,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,"Anish Athalye, Massachusetts Institute of Technology|; et al.|Nicholas Carlini, University of California, Berkeley|David Wagner, University of California, Berkeley",https://www.semanticscholar.org/paper/651adaa058f821a890f2c5d1053d69eb481a8352,"We identify obfuscated gradients as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat optimization-based attacks, we find defenses relying on this effect can be circumvented. 
For each of the three types of obfuscated gradients we discover, we describe indicators of defenses exhibiting this effect and develop attack techniques to overcome it. In a case study, examining all defenses accepted to ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 8 defenses relying on obfuscated gradients. Using our new attack techniques, we successfully circumvent all 7 of them."
icml,ICML (Machine Learning),2018,Delayed Impact of Fair Machine Learning,"Lydia T. Liu, University of California, Berkeley|; et al.|Sarah Dean, University of California, Berkeley|Esther Rolf, University of California, Berkeley|Max Simchowitz, University of California, Berkeley|Moritz Hardt, University of California, Berkeley",https://www.semanticscholar.org/paper/4f2baff3195b6fc43a38e3e869496dab9fe9dbc3,"Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. 
We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. 
We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. 
Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs."
icml,ICML (Machine Learning),2017,Understanding Black-box Predictions via Influence Functions,"Pang Wei Koh & Percy Liang, Stanford University",https://www.semanticscholar.org/paper/08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1,"How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks."
icml,ICML (Machine Learning),2016,Pixel Recurrent Neural Networks,"Aaron Van den Oord, Google|; et al.|Nal Kalchbrenner, Google|Koray Kavukcuoglu, Google",https://www.semanticscholar.org/paper/41f1d50c85d3180476c4c7b3eea121278b0d8474,"Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent."
icml,ICML (Machine Learning),2016,Dueling Network Architectures for Deep Reinforcement Learning,"Ziyu Wang, Google|; et al.|Tom Schaul, Google|Matteo Hessel, Google|Hado van Hasselt, Google|Marc Lanctot, Google|Nando de Freitas, University of Oxford",https://www.semanticscholar.org/paper/4c05d7caa357148f0bbd61720bdd35f0bc05eb81,"In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
icml,ICML (Machine Learning),2016,Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling,"Christopher De Sa, Stanford University|; et al.|Chris Re, Stanford University|Kunle Olukotun, Stanford University",https://www.semanticscholar.org/paper/5acd38bf0d012d569099eee391358ced154fe144,"Gibbs sampling is a Markov chain Monte Carlo technique commonly used for estimating marginal distributions. To speed up Gibbs sampling, there has recently been interest in parallelizing it by executing asynchronously. While empirical results suggest that many models can be efficiently sampled asynchronously, traditional Markov chain analysis does not apply to the asynchronous case, and thus asynchronous Gibbs sampling is poorly understood. In this paper, we derive a better understanding of the two main challenges of asynchronous Gibbs: bias and mixing time. We show experimentally that our theoretical results match practical outcomes."
icml,ICML (Machine Learning),2015,Optimal and Adaptive Algorithms for Online Boosting,"Alina Beygelzimer, Yahoo! Research|; et al.|Satyen Kale, Yahoo! Research|Haipeng Luo, Princeton University",https://www.semanticscholar.org/paper/6eb5c71aeaf2588a965ad48b48ae827988c4a061,"We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. The second algorithm is adaptive and parameter-free, albeit not optimal."
icml,ICML (Machine Learning),2015,A Nearly-Linear Time Framework for Graph-Structured Sparsity,"Chinmay Hegde, Massachusetts Institute of Technology|; et al.|Piotr Indyk, Massachusetts Institute of Technology|Ludwig Schmid, Massachusetts Institute of Technology",https://www.semanticscholar.org/paper/e74a1e87a0cbac606a81e9b5f8695d828c07f966,"We introduce a framework for sparsity structures defined via graphs. Our approach is flexible and generalizes several previously studied sparsity models. Moreover, we provide efficient projection algorithms for our sparsity model that run in nearly-linear time. In the context of sparse recovery, we show that our framework achieves an information-theoretically optimal sample complexity for a wide range of parameters. We complement our theoretical analysis with experiments demonstrating that our algorithms also improve on prior work in practice."
icml,ICML (Machine Learning),2014,Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis,"Jian Tang, Peking University|; et al.|Zhaoshi Meng, University of Michigan|XuanLong Nguyen, University of Michigan|Qiaozhu Mei, University of Michigan|Ming Zhang, Peking University",https://www.semanticscholar.org/paper/9975e9b3c1ab964f86bf4a553ce23cec43b567bc,"Topic models such as the latent Dirichlet allocation (LDA) have become a standard staple in the modeling toolbox of machine learning. They have been applied to a vast variety of data sets, contexts, and tasks to varying degrees of success. However, to date there is almost no formal theory explicating the LDA's behavior, and despite its familiarity there is very little systematic analysis of and guidance on the properties of the data that affect the inferential performance of the model. This paper seeks to address this gap, by providing a systematic analysis of factors which characterize the LDA's performance. We present theorems elucidating the posterior contraction rates of the topics as the amount of data increases, and a thorough supporting empirical study using synthetic and real data sets, including news and web-based articles and tweet messages. Based on these results we provide practical guidance on how to identify suitable data sets for topic models, and how to specify particular model parameters."
icml,ICML (Machine Learning),2013,Vanishing Component Analysis,"Roi Livni, The Hebrew University of Jerusalum|; et al.|David Lehavi, Hewlett Packard Labs|Sagi Schein, Hewlett Packard Labs|Hila Nachlieli, Hewlett Packard Labs|Shai Shalev Shwartz, The Hebrew University of Jerusalum|Amir Globerson, The Hebrew University of Jerusalum",https://www.semanticscholar.org/paper/3e15ee53cbe47892b5991c8fb36fd65a3071ca0a,"The vanishing ideal of a set of points, S ⊂ Rn, is the set of all polynomials that attain the value of zero on all the points in S. Such ideals can be compactly represented using a small set of polynomials known as generators of the ideal. Here we describe and analyze an efficient procedure that constructs a set of generators of a vanishing ideal. Our procedure is numerically stable, and can be used to find approximately vanishing polynomials. The resulting polynomials capture nonlinear structure in data, and can for example be used within supervised learning. Empirical comparison with kernel methods show that our method constructs more compact classifiers with comparable accuracy."
icml,ICML (Machine Learning),2013,Fast Semidifferential-based Submodular Function Optimization,"Rishabh Iyer, University of Washington|; et al.|Stefanie Jegelka, University of California, Berkeley|Jeff Bilmes, University of Washington",https://www.semanticscholar.org/paper/15c1191b6eeaa10ff6c70a3fa01162c4482f93af,"We present a practical and powerful new framework for both unconstrained and constrained submodular function optimization based on discrete semidifferentials (sub- and super-differentials). The resulting algorithms, which repeatedly compute and then efficiently optimize submodular semigradients, offer new and generalize many old methods for submodular optimization. Our approach, moreover, takes steps towards providing a unifying paradigm applicable to both submodular minimization and maximization, problems that historically have been treated quite distinctly. The practicality of our algorithms is important since interest in submodularity, owing to its natural and wide applicability, has recently been in ascendance within machine learning. We analyze theoretical properties of our algorithms for minimization and maximization, and show that many state-of-the-art maximization algorithms are special cases. Lastly, we complement our theoretical analyses with supporting empirical experiments."
icml,ICML (Machine Learning),2012,Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring,"Sungjin Ahn, University of California, Irvine|; et al.|Anoop Korattikara, University of California, Irvine|Max Welling, University of California, Irvine",https://www.semanticscholar.org/paper/57f3faa06d215481d04238a3c6ee75828863d6e4,"In this paper we address the following question: ""Can we approximately sample from a Bayesian posterior distribution if we are only allowed to touch a small mini-batch of data-items for every sample we generate?"". An algorithm based on the Langevin equation with stochastic gradients (SGLD) was previously proposed to solve this, but its mixing rate was slow. By leveraging the Bayesian Central Limit Theorem, we extend the SGLD algorithm so that at high mixing rates it will sample from a normal approximation of the posterior, while for slow mixing rates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a bonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic gradients) and as such an efficient optimizer during burn-in."
icml,ICML (Machine Learning),2011,Computational Rationalization: The Inverse Equilibrium Problem,"Kevin Waugh, Carnegie Mellon University|; et al.|Brian Ziebart, Carnegie Mellon University|Drew Bagnell, Carnegie Mellon University",https://www.semanticscholar.org/paper/85402ce171bc0ab960b0e50d9735ddb1cfcce64a,"Modeling the purposeful behavior of imperfect agents from a small number of observations is a challenging task. When restricted to the single-agent decision-theoretic setting, inverse optimal control techniques assume that observed behavior is an approximately optimal solution to an unknown decision problem. These techniques learn a utility function that explains the example behavior and can then be used to accurately predict or imitate future behavior in similar observed or unobserved situations. 
In this work, we consider similar tasks in competitive and cooperative multi-agent domains. Here, unlike single-agent settings, a player cannot myopically maximize its reward; it must speculate on how the other agents may act to influence the game's outcome. Employing the game-theoretic notion of regret and the principle of maximum entropy, we introduce a technique for predicting and generalizing behavior."
icml,ICML (Machine Learning),2010,Hilbert Space Embeddings of Hidden Markov Models,"Le Song, Carnegie Mellon University|; et al.|Byron Boots, Carnegie Mellon University|Sajid M. Siddiqi, Google|Geoffrey Gordon, Carnegie Mellon University|Alex Smola, Yahoo! Research",https://www.semanticscholar.org/paper/83610a58a7580a5451f4a123b025714e2814ed74,"Hidden Markov Models (HMMs) are important tools for modeling sequence data. However, they are restricted to discrete latent states, and are largely restricted to Gaussian and discrete observations. And, learning algorithms for HMMs have predominantly relied on local search heuristics, with the exception of spectral methods such as those described below. We propose a nonparametric HMM that extends traditional HMMs to structured and non-Gaussian continuous distributions. Furthermore, we derive a local-minimum-free kernel spectral algorithm for learning these HMMs. We apply our method to robot vision data, slot car inertial sensor data and audio event classification data, and show that in these applications, embedded HMMs exceed the previous state-of-the-art performance."
icml,ICML (Machine Learning),2009,Structure preserving embedding,"Blake Shaw & Tony Jebara, Columbia University",https://www.semanticscholar.org/paper/df30fe0aeac5a530c9499598251a3854fe45ee94,"Structure Preserving Embedding (SPE) is an algorithm for embedding graphs in Euclidean space such that the embedding is low-dimensional and preserves the global topological properties of the input graph. Topology is preserved if a connectivity algorithm, such as k-nearest neighbors, can easily recover the edges of the input graph from only the coordinates of the nodes after embedding. SPE is formulated as a semidefinite program that learns a low-rank kernel matrix constrained by a set of linear inequalities which captures the connectivity structure of the input graph. Traditional graph embedding algorithms do not preserve structure according to our definition, and thus the resulting visualizations can be misleading or less informative. SPE provides significant improvements in terms of visualization and lossless compression of graphs, outperforming popular methods such as spectral embedding and Laplacian eigen-maps. We find that many classical graphs and networks can be properly embedded using only a few dimensions. Furthermore, introducing structure preserving constraints into dimensionality reduction algorithms produces more accurate representations of high-dimensional data."
icml,ICML (Machine Learning),2008,SVM Optimization: Inverse Dependence on Training Set Size,"Shai Shalev-Shwartz & Nathan Srebro, Toyota Technological Institute at Chicago",https://www.semanticscholar.org/paper/7c103834c918d18b79a1794f26c3422435768fcd,"We discuss how the runtime of SVM optimization should decrease as the size of the training data increases. We present theoretical and empirical results demonstrating how a simple subgradient descent approach indeed displays such behavior, at least for linear kernels."
icml,ICML (Machine Learning),2007,Information-theoretic metric learning,"Jason V. Davis, University of Texas at Austin|; et al.|Brian Kulis, University of Texas at Austin|Prateek Jain, University of Texas at Austin|Suvrit Sra, University of Texas at Austin|Inderjit S. Dhillon, University of Texas at Austin",https://www.semanticscholar.org/paper/b81381d17baf6750c09bd58e96f4660d25be9225,"In this paper, we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function. We express this problem as a particular Bregman optimization problem---that of minimizing the LogDet divergence subject to linear constraints. Our resulting algorithm has several advantages over existing methods. First, our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function. Second, it is fast and scalable. Unlike most existing methods, no eigenvalue computations or semi-definite programming are required. We also present an online version and derive regret bounds for the resulting algorithm. Finally, we evaluate our method on a recent error reporting system for software called Clarify, in the context of metric learning for nearest neighbor classification, as well as on standard data sets."
icml,ICML (Machine Learning),2006,Trading convexity for scalability,"Ronan Collobert, NEC Labs America|; et al.|Fabian Sinz, NEC Labs America|Jason Weston, NEC Labs America|Léon Bottou, NEC Labs America",https://www.semanticscholar.org/paper/33fd991b4e05becfdec93585d25b6369a0519133,"Convex learning algorithms, such as Support Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis. However, in this work we show how non-convexity can provide scalability advantages over convexity. We show how concave-convex programming can be applied to produce (i) faster SVMs where training errors are no longer support vectors, and (ii) much faster Transductive SVMs."
icml,ICML (Machine Learning),2005,A support vector method for multivariate performance measures,"Thorsten Joachims, Cornell University",https://www.semanticscholar.org/paper/175c1bb60ee46dac56d942ef8c7339977b4ebb0e,"This paper presents a Support Vector Method for optimizing multivariate nonlinear performance measures like the F1-score. Taking a multivariate prediction approach, we give an algorithm with which such multivariate SVMs can be trained in polynomial time for large classes of potentially non-linear performance measures, in particular ROCArea and all measures that can be computed from the contingency table. The conventional classification SVM arises as a special case of our method."
icml,ICML (Machine Learning),1999,Least-Squares Temporal Difference Learning,"Justin A. Boyan, Ames Research Center",https://www.semanticscholar.org/paper/55c7cb8ca85c751f7a418ae06143d9f3473ce526,"TD( ) is a popular family of algorithms for approximate policy evaluation in large MDPs. TD( ) works by incrementally updating the value function after each observed transition. It has two major drawbacks: it makes ine cient use of data, and it requires the user to manually tune a stepsize schedule for good performance. For the case of linear value function approximations and = 0, the Least-Squares TD (LSTD) algorithm of Bradtke and Barto (Bradtke and Barto, 1996) eliminates all stepsize parameters and improves data e ciency. This paper extends Bradtke and Barto's work in three signi cant ways. First, it presents a simpler derivation of the LSTD algorithm. Second, it generalizes from = 0 to arbitrary values of ; at the extreme of = 1, the resulting algorithm is shown to be a practical formulation of supervised linear regression. Third, it presents a novel, intuitive interpretation of LSTD as a model-based reinforcement learning technique."
iccv,ICCV (Computer Vision),2019,SinGAN: Learning a Generative Model from a Single Natural Image,"Tamar Rott Shaham, Technion – Israel Institute of Technology|; et al.|Tali Dekel, Google Research|Tomar Michaeli, Technion – Israel Institute of Technology",https://www.semanticscholar.org/paper/SinGAN%3A-Learning-a-Generative-Model-from-a-Single-Shaham-Dekel/ccaf15d4ad006171061508ca0a99c73814671501,"We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks."
iccv,ICCV (Computer Vision),2017,Mask R-CNN,"Kaiming He, Facebook AI Research|; et al.|Georgia Gkioxari, Facebook AI Research|Piotr Dollar, Facebook AI Research|Ross Girshick, Facebook AI Research",https://www.semanticscholar.org/paper/ea99a5535388196d0d44be5b4d7dd02029a43bb2,"We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available."
iccv,ICCV (Computer Vision),2015,Deep Neural Decision Forests,"Peter Kontschieder, Microsoft Research|; et al.|Madalina Fiterau, Carnegie Mellon University|Antonio Criminisi, Microsoft Research|Samuel Rota Bulò, Microsoft Research",https://www.semanticscholar.org/paper/544998db166c047c70a61c5a5c54d10c5879ecf1,"We present Deep Neural Decision Forests - a novel approach that unifies classification trees with the representation learning functionality known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find on-par or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84%/6.38% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67% error obtained by the best GoogLeNet architecture (7 models, 144 crops)."
iccv,ICCV (Computer Vision),2013,From Large Scale Image Categorization to Entry-Level Categories,"Vicente Ordonez, University of North Carolina at Chapel Hill|; et al.|Jia Deng, Stanford University|Yejin Choi, Stony Brook University|Alexander Berg, University of North Carolina at Chapel Hill|Tamara Berg, University of North Carolina at Chapel Hill",https://www.semanticscholar.org/paper/3bfeecf2aa26efe211985e19a967b2cb28012482,"Entry level categories - the labels people will use to name an object - were originally defined and studied by psychologists in the 1980s. In this paper we study entry-level categories at a large scale and learn the first models for predicting entry-level categories for images. Our models combine visual recognition predictions with proxies for word ""naturalness"" mined from the enormous amounts of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people. We also learn mappings between concepts predicted by existing visual recognition systems and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval."
iccv,ICCV (Computer Vision),2011,Relative Attributes,"Devi Parikh, Toyota Technological Institute at Chicago|Kristen Grauman, University of Texas at Austin",https://www.semanticscholar.org/paper/23e568fcf0192e4ff5e6bed7507ee5b9e6c43598,"Human-nameable visual “attributes” can benefit various recognition tasks. However, existing techniques restrict these properties to categorical labels (for example, a person is ‘smiling’ or not, a scene is ‘dry’ or not), and thus fail to capture more general semantic relationships. We propose to model relative attributes. Given training data stating how object/scene categories relate according to different attributes, we learn a ranking function per attribute. The learned ranking functions predict the relative strength of each property in novel images. We then build a generative model over the joint space of attribute ranking outputs, and propose a novel form of zero-shot learning in which the supervisor relates the unseen object category to previously seen objects via attributes (for example, ‘bears are furrier than giraffes’). We further show how the proposed relative attributes enable richer textual descriptions for new images, which in practice are more precise for human interpretation. We demonstrate the approach on datasets of faces and natural scenes, and show its clear advantages over traditional binary attribute prediction for these new tasks."
iccv,ICCV (Computer Vision),2009,Discriminative models for multi-class object layout,"Chaitanya Desai, University of California, Irvine|; et al.|Deva Ramanan, University of California, Irvine|Charless Fowlkes, University of California, Irvine",https://www.semanticscholar.org/paper/c90a0b7011bece4d7a5af1241410023273aa033e,"Many state-of-the-art approaches for object recognition reduce the problem to a 0-1 classification task. This allows one to leverage sophisticated machine learning techniques for training classifiers from labeled examples. However , these models are typically trained independently for each class using positive and negative examples cropped from images. At test-time, various post-processing heuris-tics such as non-maxima suppression (NMS) are required to reconcile multiple detections within and between different classes for each image. Though crucial to good performance on benchmarks, this post-processing is usually defined heuristically. We introduce a unified model for multi-class object recognition that casts the problem as a structured prediction task. Rather than predicting a binary label for each image window independently, our model simultaneously predicts a structured labeling of the entire image (Fig. 1). Our model learns statistics that capture the spatial arrangements of various object classes in real images, both in terms of which arrangements to suppress through NMS and which arrangements to favor through spatial co-occurrence statistics. We formulate parameter estimation in our model as a max-margin learning problem. Given training images with The Marr Prize is awarded to the best paper(s) at the biannual flagship vision conference, the IEEE International Conference on Computer Vision (ICCV). This paper is an extended and re-reviewed journal version of the 2009 prize-winning conference paper. ground-truth object locations, we show how to formulate learning as a convex optimization problem. We employ the cutting plane algorithm of Joachims et al. (Mach. Learn. 2009) to efficiently learn a model from thousands of training images. We show state-of-the-art results on the PASCAL VOC benchmark that indicate the benefits of learning a global model encapsulating the spatial layout of multiple object classes (a preliminary version of this work appeared in ICCV 2009, Desai et al., IEEE international conference on computer vision, 2009)."
iccv,ICCV (Computer Vision),2007,Population Shape Regression From Random Design Data,"Bradley Davis, University of North Carolina at Chapel Hill|; et al.|P. Thomas Fletcher, University of Utah|Elizabeth Bullitt, University of North Carolina at Chapel Hill|Sarang Joshi, University of Utah",https://www.semanticscholar.org/paper/163088df67df32b51aedded945195666db2ce8d0,"Regression analysis is a powerful tool for the study of changes in a dependent variable as a function of an independent regressor variable, and in particular it is applicable to the study of anatomical growth and shape change. When the underlying process can be modeled by parameters in a Euclidean space, classical regression techniques (Hardle, Applied Nonparametric Regression, 1990; Wand and Jones, Kernel Smoothing, 1995) are applicable and have been studied extensively. However, recent work suggests that attempts to describe anatomical shapes using flat Euclidean spaces undermines our ability to represent natural biological variability (Fletcher et al., IEEE Trans. Med. Imaging 23(8), 995–1005, 2004; Grenander and Miller, Q. Appl. Math. 56(4), 617–694, 1998).In this paper we develop a method for regression analysis of general, manifold-valued data. Specifically, we extend Nadaraya-Watson kernel regression by recasting the regression problem in terms of Fréchet expectation. Although this method is quite general, our driving problem is the study anatomical shape change as a function of age from random design image data.We demonstrate our method by analyzing shape change in the brain from a random design dataset of MR images of 97 healthy adults ranging in age from 20 to 79 years. To study the small scale changes in anatomy, we use the infinite dimensional manifold of diffeomorphic transformations, with an associated metric. We regress a representative anatomical shape, as a function of age, from this population."
iccv,ICCV (Computer Vision),2005,Globally Optimal Estimates for Geometric Reconstruction Problems,"Fredrik Kahl, Lund University|Didier Henrion, Laboratory for Analysis and Architecture of Systems",https://www.semanticscholar.org/paper/daf03088b494c8ab508e99f12437883a4e86585d,"We introduce a framework for computing statistically optimal estimates of geometric reconstruction problems. While traditional algorithms often suffer from either local minima or non-optimality—or a combination of both—we pursue the goal of achieving global solutions of the statistically optimal cost-function.Our approach is based on a hierarchy of convex relaxations to solve non-convex optimization problems with polynomials. These convex relaxations generate a monotone sequence of lower bounds and we show how one can detect whether the global optimum is attained at a given relaxation. The technique is applied to a number of classical vision problems: triangulation, camera pose, homography estimation and last, but not least, epipolar geometry estimation. Experimental validation on both synthetic and real data is provided. In practice, only a few relaxations are needed for attaining the global optimum."
iccv,ICCV (Computer Vision),2003,Image-based Rendering using Image-based Priors,"Andrew Fitzgibbon, University of Oxford|; et al.|Yonatan Wexler, Weizmann Institute of Science|Andrew Zisserman, University of Oxford",https://www.semanticscholar.org/paper/d78b523c9bafc73da74f75756acd7eb6b4c0076d,"For this report, the view synthesis algorithm from the paper of the same title by Fitzgibbon et al. [1] was implemented. In this report, the geometric and probabilistic background of the algorithm, as well as necessary optimizations required to make the problem more tractable, are succinctly detailed. Results are then presented and analyzed. It was found that although the use of the texture prior improves the resulting rendered images, the initial photoconsistent estimate without use of the prior is of very good visual quality. There is still however a lot of room for improvements in terms of computational performance."
iccv,ICCV (Computer Vision),2003,Detecting Pedestrians using Patterns of Motion and Appearance,"Paul Viola, Microsoft Research|; et al.|Michael J. Jones, Mitsubishi Electric Research Laboratories|Daniel Snow, Mitsubishi Electric Research Laboratories",https://www.semanticscholar.org/paper/4521ac382772e90607c23af1cff34a04df37e6c4,"This paper describes a pedestrian detection system that integrates image intensity information with motion information. We use a detection style algorithm that scans a detector over two consecutive frames of a video sequence. The detector is trained (using AdaBoost) to take advantage of both motion and appearance information to detect a walking person. Past approaches have built detectors based on appearance information, but ours is the first to combine both sources of information in a single detector. The implementation described runs at about 4 frames/second, detects pedestrians at very small scales (as small as 20/spl times/15 pixels), and has a very low false positive rate. Our approach builds on the detection work of Viola and Jones. Novel contributions of this paper include: i) development of a representation of image motion which is extremely efficient, and ii) implementation of a state of the art pedestrian detection system which operates on low resolution images under difficult conditions (such as rain and snow)."
iccv,ICCV (Computer Vision),2003,"Image Parsing: Unifying Segmentation, Detection and Recognition","Zhuowen Tu, University of California, Los Angeles|; et al.|Xiangrong Chen, University of California, Los Angeles|Alan L. Yuille, University of California, Los Angeles|Song-Chun Zhu, University of California, Los Angeles",https://www.semanticscholar.org/paper/cca9200d9da958b7f90eab901b2f30c04f1e0e9c,"We propose a general framework for parsing images into regions and objects. In this framework, the detection and recognition of objects proceed simultaneously with image segmentation in a competitive and cooperative manner. We illustrate our approach on natural images of complex city scenes where the objects of primary interest are faces and text. This method makes use of bottom-up proposals combined with top-down generative models using the data driven Markov chain Monte Carlo (DDMCMC) algorithm, which is guaranteed to converge to the optimal estimate asymptotically. More precisely, we define generative models for faces, text, and generic regions- e.g. shading, texture, and clutter. These models are activated by bottom-up proposals. The proposals for faces and text are learnt using a probabilistic version of AdaBoost. The DDMCMC combines reversible jump and diffusion dynamics to enable the generative models to explain the input images in a competitive and cooperative manner. Our experiments illustrate the advantages and importance of combining bottom-up and top-down models and of performing segmentation and object detection/recognition simultaneously."
iccv,ICCV (Computer Vision),2001,Probabilistic Tracking with Exemplars in a Metric Space,"Kentaro Toyama & Andrew Blake, Microsoft Research",https://www.semanticscholar.org/paper/7bfe222d83e9dd9117bc4f8508962a01fb0b1626,"A new exemplar-based, probabilistic paradigm for visual tracking is presented. Probabilistic mechanisms are attractive because they handle fusion of information, especially temporal fusion, in a principled manner. Exemplars are selected representatives of raw training data, used here to represent probabilistic mixture distributions of object configurations. Their use avoids tedious hand-construction of object models and problems with changes of topology. Using exemplars in place of a parameterized model poses several challenges, addressed here with what we call the ""Metric Mixture"" (M/sup 2/) approach. The M/sup 2/ model has several valuable properties. Principally, it provides alternatives to standard learning algorithms by allowing the use of metrics that are not embedded in a vector space. Secondly, it uses a noise model that is learned from training data. Lastly, it eliminates any need for an assumption of probabilistic pixelwise independence. Experiments demonstrate the effectiveness of the M/sup 2/ model in two domains tracking walking people using chamfer distances on binary edge images and tracking mouth movements by means of a shuffle distance."
iccv,ICCV (Computer Vision),2001,The Space of All Stereo Images,"Steven Seitz, University of Washington",https://www.semanticscholar.org/paper/f868eb0cfaad536f17ac018377c75ae320ee2027,"A theory of stereo image formation is presented that enables a complete classification of all possible stereo views, including non-perspective varieties. Towards this end, the notion of epipolar geometry is generalized to apply to multiperspective images. It is shown that any stereo pair must consist of rays lying on one of three varieties of quadric surfaces. A unified representation is developed to model all classes of stereo views, based on the concept of a quadric view. The benefits include a unified treatment of projection and triangulation operations for all stereo views. The framework is applied to derive new types of stereo image representations with unusual and useful properties. Experimental examples of these images are constructed and used to obtain 3D binocular object reconstructions."
iccv,ICCV (Computer Vision),1999,Euclidean Reconstruction and Reprojection up to Subgroups,"Yi Ma, University of California, Berkeley|; et al.|Stefano Soatto, Washington University in St. Louis|Jana Kosecka, University of California, Berkeley|Shankar Sastry, University of California, Berkeley",https://www.semanticscholar.org/paper/fff8169a57d8c5b398c7190291bea52928a84dce,"The necessary and sufficient conditions for being able to estimate scene structure, motion and camera calibration from a sequence of images are very rarely satisfied in practice. What exactly can be estimated in sequences of practical importance, when such conditions are not satisfied? In this paper we give a complete answer to this question. For every camera motion that fails to meet the conditions, we give explicit formulas for the ambiguities in the reconstructed scene, motion and calibration. Such a characterization is crucial both for designing robust estimation algorithms (that do not try to recover parameters that cannot be recovered), and for generating novel views of the scene by controlling the vantage point. To this end, we characterize explicitly all the vantage points that give rise to a valid Euclidean reprojection regardless of the ambiguity in the reconstruction. We also characterize vantage points that generate views that are altogether invariant to the ambiguity. All the results are presented using simple notation that involves no tensors nor complex projective geometry, and should be accessible with basic background in linear algebra."
iccv,ICCV (Computer Vision),1999,A Theory of Shape by Space Carving,"Kiriakos Kutulakos, University of Rochester|Steven Seitz, Carnegie Mellon University",https://www.semanticscholar.org/paper/0f6de146325a19d23b5b2cc3607a9d2c8d8f22a8,"In this paper we consider the problem of computing the 3D shape of an unknown, arbitrarily-shaped scene from multiple photographs taken at known but arbitrarily-distributed viewpoints. By studying the equivalence class of all 3D shapes that reproduce the input photographs, we prove the existence of a special member of this class, the photo hull, that (1) can be computed directly from photographs of the scene, and (2) subsumes all other members of this class. We then give a provably-correct algorithm called Space Carving, for computing this shape and present experimental results on complex real-world scenes. The approach is designed to (1) build photorealistic shapes that accurately model scene appearance from a wide range of viewpoints, and (2) account for the complex interactions between occlusion, parallax, shading, and their effects on arbitrary views of a 3D scene."
iccv,ICCV (Computer Vision),1998,Self-Calibration and Metric Reconstruction in spite of Varying and Unknown Internal Camera Paramet...,"Marc Pollefeys, Katholieke Universiteit Leuven|; et al.|Reinhard Koch, Katholieke Universiteit Leuven|Luc Van Gool, Katholieke Universiteit Leuven",https://www.semanticscholar.org/paper/84544813dbe31fd317c2942df9bad8f63bf506e7,In this paper the feasibility of self-calibration in the presence of varying internal camera parameters is under investigation. A self-calibration method is presented which efficiently deals with all kinds of constraints on the internal camera parameters. Within this framework a practical method is proposed which can retrieve metric reconstruction from image sequences obtained with uncalibrated zooming/focusing cameras. The feasibility of the approach is illustrated on real and synthetic examples.
iccv,ICCV (Computer Vision),1998,The Problem of Degeneracy in Structure and Motion Recovery from Uncalibrated Image Sequences,"Phil Torr, Microsoft Research|; et al.|Andrew Fitzgibbon, University of Oxford|Andrew Zisserman, University of Oxford",https://www.semanticscholar.org/paper/882693f7a8e674472d9c605a35b73f498cbde82f,"The aim of this work is the recovery of 3D structure and camera projection matrices for each frame of an uncalibrated image sequence. In order to achieve this, correspondences are required throughout the sequence. A significant and successful mechanism for automatically establishing these correspondences is by the use of geometric constraints arising from scene rigidity. However, problems arise with such geometry guided matching if general viewpoint and general structure are assumed whilst frames in the sequence and/or scene structure do not conform to these assumptions. Such cases are termed degenerate.In this paper we describe two important cases of degeneracy and their effects on geometry guided matching. The cases are a motion degeneracy where the camera does not translate between frames, and a structure degeneracy where the viewed scene structure is planar. The effects include the loss of correspondences due to under or over fitting of geometric models estimated from image data, leading to the failure of the tracking method. These degeneracies are not a theoretical curiosity, but commonly occur in real sequences where models are statistically estimated from image points with measurement error.We investigate two strategies for tackling such degeneracies: the first uses a statistical model selection test to identify when degeneracies occur: the second uses multiple motion models to overcome the degeneracies. The strategies are evaluated on real sequences varying in motion, scene type, and length from 13 to 120 frames."
acl,ACL (Natural Language Processing),2019,Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts,"Rui Xia & Zixiang Ding, Nanjing University of Science and Technology",https://www.semanticscholar.org/paper/Emotion-Cause-Pair-Extraction%3A-A-New-Task-to-in-Xia-Ding/c75e65325995e2eff147af209f5dd0b5edf7ff38,"Emotion cause extraction (ECE), the task aimed at extracting the potential causes behind certain emotions in text, has gained much attention in recent years due to its wide applications. However, it suffers from two shortcomings: 1) the emotion must be annotated before cause extraction in ECE, which greatly limits its applications in real-world scenarios; 2) the way to first annotate emotion and then extract the cause ignores the fact that they are mutually indicative. In this work, we propose a new task: emotion-cause pair extraction (ECPE), which aims to extract the potential pairs of emotions and corresponding causes in a document. We propose a 2-step approach to address this new ECPE task, which first performs individual emotion extraction and cause extraction via multi-task learning, and then conduct emotion-cause pairing and filtering. The experimental results on a benchmark emotion cause corpus prove the feasibility of the ECPE task as well as the effectiveness of our approach."
acl,ACL (Natural Language Processing),2018,Finding syntax in human encephalography with beam search,"John Hale, Cornell University|; et al.|Chris Dyer, DeepMind|Adhiguna Kuncoro, University of Oxford|Jonathan R. Brennan, University of Michigan",https://www.semanticscholar.org/paper/8981a71bb7dc6d0e9a25e24fc742e9cd5a511135,"Recurrent neural network grammars (RNNGs) are generative models of (tree,string) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension."
acl,ACL (Natural Language Processing),2017,Probabilistic Typology: Deep Generative Models of Vowel Inventories,"Ryan Cotterell & Jason Eisner, Johns Hopkins University",https://www.semanticscholar.org/paper/6fad97c4fe0cfb92478d8a17a4e6aaa8637d8222,"Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages."
acl,ACL (Natural Language Processing),2016,Finding Non-Arbitrary Form-Meaning Systematicity Using String-Metric Learning for Kernel Regression,"E. Darío Gutiérrez, University of California, Berkeley|; et al.|Roger Levy, Massachusetts Institute of Technology|Benjamin K. Bergen, University of California, San Diego",https://www.semanticscholar.org/paper/14a0a960c05ad064f0cd40ff81ee059772aaff8b,"Arbitrariness of the sign—the notion that the forms of words are unrelated to their meanings—is an underlying assumption of many linguistic theories. Two lines of research have recently challenged this assumption, but they produce differing characterizations of non-arbitrariness in language. Behavioral and corpus studies have confirmed the validity of localized form-meaning patterns manifested in limited subsets of the lexicon. Meanwhile, global (lexicon-wide) statistical analyses instead find diffuse form-meaning systematicity across the lexicon as a whole. We bridge the gap with an approach that can detect both local and global formmeaning systematicity in language. In the kernel regression formulation we introduce, form-meaning relationships can be used to predict words’ distributional semantic vectors from their forms. Furthermore, we introduce a novel metric learning algorithm that can learn weighted edit distances that minimize kernel regression error. Our results suggest that the English lexicon exhibits far more global form-meaning systematicity than previously discovered, and that much of this systematicity is focused in localized formmeaning patterns."
acl,ACL (Natural Language Processing),2015,Learning Dynamic Feature Selection for Fast Sequential Prediction,"Emma Strubell, University of Massachusetts Amherst|; et al.|Luke Vilnis, University of Massachusetts Amherst|Kate Silverstein, University of Massachusetts Amherst|Andrew McCallum, University of Massachusetts Amherst",https://www.semanticscholar.org/paper/11cb642ff779f2c0976a647e32a59ef2c9d04a8d,"We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. Our approach is simpler and better suited to NLP than other related cascade methods. We present experiments in left-to-right part-of-speech tagging, named entity recognition, and transition-based dependency parsing. On the typical benchmarking datasets we can preserve POS tagging accuracy above 97% and parsing LAS above 88.5% both with over a five-fold reduction in run-time, and NER F1 above 88 with more than 2x increase in speed."
acl,ACL (Natural Language Processing),2015,Improving Evaluation of Machine Translation Quality Estimation,"Yvette Graham, Trinity College Dublin",https://www.semanticscholar.org/paper/7caddbbf06e9aa331a96983db4555c47f24961fa,"Quality estimation evaluation commonly takes the form of measurement of the error that exists between predictions and gold standard labels for a particular test set of translations. Issues can arise during comparison of quality estimation prediction score distributions and gold label distributions, however. In this paper, we provide an analysis of methods of comparison and identify areas of concern with respect to widely used measures, such as the ability to gain by prediction of aggregate statistics specific to gold label distributions or by optimally conservative variance in prediction score distributions. As an alternative, we propose the use of the unit-free Pearson correlation, in addition to providing an appropriate method of significance testing improvements over a baseline. Components of WMT-13 and WMT-14 quality estimation shared tasks are replicated to reveal substantially increased conclusivity in system rankings, including identification of outright winners of tasks."
acl,ACL (Natural Language Processing),2014,Fast and Robust Neural Network Joint Models for Statistical Machine Translation,"Jacob Devlin, BBN Technologies|; et al.|Rabih Zbib, BBN Technologies|Zhongqiang Huang, BBN Technologies|Thomas Lamar, BBN Technologies|Richard Schwartz, BBN Technologies|John Makhoul, BBN Technologies",https://www.semanticscholar.org/paper/0894b06cff1cd0903574acaa7fcf071b144ae775,"Recent work has shown success in using neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexicalized and can be integrated into any MT decoder. We also present several variations of the NNJM which provide significant additive improvements."
acl,ACL (Natural Language Processing),2013,Grounded Language Learning from Video Described with Sentences,"Haonan Yu & Jeffrey Mark Siskind, Purdue University",https://www.semanticscholar.org/paper/96a0320ef14877038906947b684011cf7378c440,"We present a method that learns representations for word meanings from short video clips paired with sentences. Unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple complex objects in outdoor environments. Unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels, our labels are sentences containing nouns, verbs, prepositions, adjectives, and adverbs. The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts simultaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video."
acl,ACL (Natural Language Processing),2012,Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing,"Hiroyuki Shindo, NTT Communication Science Laboratories|; et al.|Yusuke Miyao, National Institute of Informatics|Akinori Fujino, NTT Communication Science Laboratories|Masaaki Nagata, NTT Communication Science Laboratories",https://www.semanticscholar.org/paper/2b920fe2d038571693bd96dafd3ed0dbadc4cb67,"We propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. We aim to provide a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. We present a novel probabilistic SR-TSG model based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers."
acl,ACL (Natural Language Processing),2011,Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections,"Dipanjan Das, Carnegie Mellon University|Slav Petrov, Google",https://www.semanticscholar.org/paper/343733a063e491d234a36d3e1090a739318b3566,"We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm."
acl,ACL (Natural Language Processing),2010,Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates,"Matthew Gerber & Joyce Y. Chai, Michigan State University",https://www.semanticscholar.org/paper/d8259bcbe9cb0cf5bad6ea25645f4407fc544a1c,"Despite its substantial coverage, NomBank does not account for all within-sentence arguments and ignores extra-sentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task."
acl,ACL (Natural Language Processing),2009,K-Best A* Parsing,"Adam Pauls & Dan Klein, University of California, Berkeley",https://www.semanticscholar.org/paper/5d850ff0f29debf337eba67f5e7ab57ad5fd8dd5,"A* parsing makes 1-best search efficient by suppressing unlikely 1-best items. Existing k-best extraction methods can efficiently search for top derivations, but only after an exhaustive 1-best pass. We present a unified algorithm for k-best A* parsing which preserves the efficiency of k-best extraction while giving the speed-ups of A* methods. Our algorithm produces optimal k-best parses under the same conditions required for optimality in a 1-best A* parser. Empirically, optimal k-best lists can be extracted significantly faster than with other approaches, over a range of grammar types."
acl,ACL (Natural Language Processing),2009,Concise Integer Linear Programming Formulations for Dependency Parsing,"André F.T. Martins, Telecommunications Institute|; et al.|Noah A. Smith, Carnegie Mellon University|Eric P. Xing, Carnegie Mellon University",https://www.semanticscholar.org/paper/60df78bc14265d4b87ae5993d6b5781ccf7ec5d8,"We formulate the problem of non-projective dependency parsing as a polynomial-sized integer linear program. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data. In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearly-projective parses. The model parameters are learned in a max-margin framework by employing a linear programming relaxation. We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods."
acl,ACL (Natural Language Processing),2009,Reinforcement Learning for Mapping Instructions to Actions,"S.R.K. Branavan, Massachusetts Institute of Technology|; et al.|Harr Chen, Massachusetts Institute of Technology|Luke S. Zettlemoyer, Massachusetts Institute of Technology|Regina Barzilay, Massachusetts Institute of Technology",https://www.semanticscholar.org/paper/cc1648c91ffda21bbe6e5f08f69c683588fc384c,"In this paper, we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions. We assume access to a reward function that defines the quality of the executed actions. During training, the learner repeatedly constructs action sequences for a set of documents, executes those actions, and observes the resulting reward. We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains --- Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples."
acl,ACL (Natural Language Processing),2008,A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model,"Libin Shen, BBN Technologies|; et al.|Jinxi Xu, BBN Technologies|Ralph Weischedel, BBN Technologies",https://www.semanticscholar.org/paper/248d32911670e551db4835a5a5279d2d9673ee37,"In this paper, we propose a novel string-todependency algorithm for statistical machine translation. With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model. Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set."
acl,ACL (Natural Language Processing),2008,Forest Reranking: Discriminative Parsing with Non-Local Features,"Liang Huang, University of Pennsylvania",https://www.semanticscholar.org/paper/e9c7169aba6c8ff772aba06a19a8570b1e01071f,"Conventional n-best reranking techniques often suffer from the limited scope of the nbest list, which rules out many potentially good alternatives. We instead propose forest reranking, a method that reranks a packed forest of exponentially many parses. Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank. Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank."
acl,ACL (Natural Language Processing),2007,Learning synchronous grammars for semantic parsing with lambda calculus,"Yuk Wah Wong & Raymond J. Mooney, University of Texas at Austin",https://www.semanticscholar.org/paper/c2ecc66c0e5f976b0e0d95c64ed2d1e283a2625d,"This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with �operators is learned given a set of training sentences and their correct logical forms. The resulting parser is shown to be the bestperforming system so far in a database query domain."
acl,ACL (Natural Language Processing),2006,Semantic taxonomy induction from heterogenous evidence,"Rion Snow, Stanford University|; et al.|Daniel Jurafsky, Stanford University|Andrew Y. Ng, Stanford University",https://www.semanticscholar.org/paper/93bb6228776eafa606965e21f229d548de1998eb,"We propose a novel algorithm for inducing semantic taxonomies. Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns. By contrast, our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy, using knowledge of a word's coordinate terms to help in determining its hypernyms, and vice versa. We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1). We add 10,000 novel synsets to WordNet 2.1 at 84% precision, a relative error reduction of 70% over a non-joint algorithm using the same component classifiers. Finally, we show that a taxonomy built using our algorithm shows a 23% relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs."
acl,ACL (Natural Language Processing),2005,A Hierarchical Phrase-Based Model for Statistical Machine Translation,"David Chiang, University of Maryland",https://www.semanticscholar.org/paper/ad3d2f463916784d0c14a19936c1544309a0a440,"We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system."
acl,ACL (Natural Language Processing),2004,Finding Predominant Word Senses in Untagged Text,"Diana McCarthy, University of Sussex|; et al.|Rob Koeling, University of Sussex|Julie Weeds, University of Sussex|John Carroll, University of Sussex",https://www.semanticscholar.org/paper/1aac9a51700b4f548ed4d406d3987c8008876521,"In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of hand-tagged data. Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL-2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domain-specific corpora."
acl,ACL (Natural Language Processing),2003,Accurate Unlexicalized Parsing,"Dan Klein & Christopher D. Manning, Stanford University",https://www.semanticscholar.org/paper/a600850ac0120cb09a0b7de7da80bb6a7a76de06,"We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F1) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize."
acl,ACL (Natural Language Processing),2003,Towards a Model of Face-to-Face Grounding,"Yukiko I. Nakano, Research Institute of Science and Technology for Society|; et al.|Gabe Reinstein, Massachusetts Institute of Technology|Tom Stocky, Massachusetts Institute of Technology|Justine Cassell, Massachusetts Institute of Technology",https://www.semanticscholar.org/paper/d2fa3412d4c9b2867f2952de9851f729761b8f47,"We investigate the verbal and nonverbal means for grounding, and propose a design for embodied conversational agents that relies on both kinds of signals to establish common ground in human-computer interaction. We analyzed eye gaze, head nods and attentional focus in the context of a direction-giving task. The distribution of nonverbal behaviors differed depending on the type of dialogue move being grounded, and the overall pattern reflected a monitoring of lack of negative feedback. Based on these results, we present an ECA that uses verbal and nonverbal grounding acts to update dialogue state."
acl,ACL (Natural Language Processing),2002,Discriminative Training and Maximum Entropy Models for Statistical Machine Translation,"Franz Josef Och & Hermann Ney, RWTH Aachen University",https://www.semanticscholar.org/paper/37fadfb6d60e83e24c72d8a90da5644b39d6e8f0,"We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine translation system is significantly improved using this approach."
acl,ACL (Natural Language Processing),2001,Immediate-Head Parsing for Language Models,"Eugene Charniak, Brown University",https://www.semanticscholar.org/paper/71093afbe9da56e599fd7af8a5d4832ad0b15ded,"Semantic Scholar extracted view of ""Immediate-Head Parsing for Language Models"" by Providen e RIe"
acl,ACL (Natural Language Processing),2001,Fast Decoding and Optimal Decoding for Machine Translation,"Ulrich Germann, University of Southern California|; et al.|Michael Jahr, Stanford University|Kevin Knight, University of Southern California|Daniel Marcu, University of Southern California|Kenji Yamada, University of Southern California",https://www.semanticscholar.org/paper/dd5514876b7e1c09b6d2f931d90bb34aa3501441,"A good decoding algorithm is critical to the success of any statistical machine translation system. The decoder's job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them). Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions. In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem."
aaai,AAAI (Artificial Intelligence),2020,WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale,"Keisuke Sakaguchi, Allen Institute for Artificial Intelligence|; et al.|Ronan Le Bras, Allen Institute for Artificial Intelligence|Chandra Bhagavatula, Allen Institute for Artificial Intelligence|Yejin Choi, University of Washington",https://www.semanticscholar.org/paper/8f7133b2e3851b09d659b91e8faa761ec206413f,"The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90% accuracy on variants of WSC. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on WinoGrande achieve 59.4-79.1%, which are 15-35% below human performance of 94.0%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - WSC (90.1%), DPR (93.1%), COPA (90.6%), KnowRef (85.6%), and Winogender (97.1%). These results have dual implications: on one hand, they demonstrate the effectiveness of WinoGrande when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation."
aaai,AAAI (Artificial Intelligence),2019,How to Combine Tree-Search Methods in Reinforcement Learning,"Yonathan Efroni, Technion – Israel Institute of Technology|; et al.|Gal Dalal, Technion – Israel Institute of Technology|Bruno Scherrer, Inria|Shie Mannor, Technion – Israel Institute of Technology",https://www.semanticscholar.org/paper/How-to-Combine-Tree-Search-Methods-in-Reinforcement-Efroni-Dalal/1cba4288144124680c23f2a3d923a1a98f9bebf1,"Finite-horizon lookahead policies are abundantly used in Reinforcement Learning and demonstrate impressive empirical success. Usually, the lookahead policies are implemented with specific planning methods such as Monte Carlo Tree Search (e.g. in AlphaZero). Referring to the planning problem as tree search, a reasonable practice in these implementations is to back up the value only at the leaves while the information obtained at the root is not leveraged other than for updating the policy. Here, we question the potency of this approach. Namely, the latter procedure is non-contractive in general, and its convergence is not guaranteed. Our proposed enhancement is straightforward and simple: use the return from the optimal tree path to back up the values at the descendants of the root. This leads to a $\gamma^h$-contracting procedure, where $\gamma$ is the discount factor and $h$ is the tree depth. To establish our results, we first introduce a notion called \emph{multiple-step greedy consistency}. We then provide convergence rates for two algorithmic instantiations of the above enhancement in the presence of noise injected to both the tree search stage and value estimation stage."
aaai,AAAI (Artificial Intelligence),2018,Memory-Augmented Monte Carlo Tree Search,"Chenjun Xiao, University of Alberta|; et al.|Jincheng Mei, University of Alberta|Martin Müller, University of Alberta",https://www.semanticscholar.org/paper/2d7fb18fa63f718866055ab494e1b0707b0bd56a,"This paper proposes and evaluates Memory-Augmented Monte Carlo Tree Search (M-MCTS), which provides a new approach to exploit generalization in online real-time search. The key idea of M-MCTS is to incorporate MCTS with a memory structure, where each entry contains information of a particular state. This memory is used to generate an approximate value estimation by combining the estimations of similar states. We show that the memory based value approximation is better than the vanilla Monte Carlo estimation with high probability under mild conditions. We evaluate MMCTS in the game of Go. Experimental results show that MMCTS outperforms the original MCTS with the same number of simulations."
aaai,AAAI (Artificial Intelligence),2017,Label-Free Supervision of Neural Networks with Physics and Domain Knowledge,"Russell Stewart & Stefano Ermon, Stanford University",https://www.semanticscholar.org/paper/2ee629820b95f311927d24570d7719bd2843f66d,"In many machine learning applications, labeled data is scarce and obtaining more labels is expensive. We introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space, rather than direct examples of input-output pairs. These constraints are derived from prior domain knowledge, e.g., from known laws of physics. We demonstrate the effectiveness of this approach on real world and simulated computer vision tasks. We are able to train a convolutional neural network to detect and track objects without any labeled examples. Our approach can significantly reduce the need for labeled training data, but introduces new challenges for encoding prior knowledge into appropriate loss functions."
aaai,AAAI (Artificial Intelligence),2016,Bidirectional Search That Is Guaranteed to Meet in the Middle,"Robert C. Holte, University of Alberta|; et al.|Ariel Felner, Ben-Gurion University of the Negev|Guni Sharon, Ben-Gurion University of the Negev|Nathan R. Sturtevant, University of Denver",https://www.semanticscholar.org/paper/d4c2696a254c88a2382b399d7c8c146f49338bf1,"We present MM, the first bidirectional heuristic search algorithm whose forward and backward searches are guaranteed to ""meet in the middle"", i.e. never expand a node beyond the solution midpoint. We also present a novel framework for comparing MM, A*, and brute-force search, and identify conditions favoring each algorithm. Finally, we present experimental results that support our theoretical analysis."
aaai,AAAI (Artificial Intelligence),2015,From Non-Negative to General Operator Cost Partitioning,"Florian Pommerening, University of Basel|; et al.|Malte Helmert, University of Basel|Gabriele Röger, University of Basel|Jendrik Seipp, University of Basel",https://www.semanticscholar.org/paper/6f45883551ecb1cb33cb30cf1f67fc3d28f4a59c,Operator cost partitioning is a well-known technique to make admissible heuristics additive by distributing the operator costs among individual heuristics. Planning tasks are usually defined with non-negative operator costs and therefore it appears natural to demand the same for the distributed costs. We argue that this requirement is not necessary and demonstrate the benefit of using general cost partitioning. We show that LP heuristics for operator-counting constraints are cost-partitioned heuristics and that the state equation heuristic computes a cost partitioning over atomic projections. We also introduce a new family of potential heuristics and show their relationship to general cost partitioning.
aaai,AAAI (Artificial Intelligence),2014,Recovering from Selection Bias in Causal and Statistical Inference,"Elias Bareinboim, University of California, Los Angeles|; et al.|Jin Tian, Iowa State University|Judea Pearl, University of California, Los Angeles",https://www.semanticscholar.org/paper/91dfdd05a99256c2293b952c3b008326b015d4eb,"Selection bias is caused by preferential exclusion of units from the samples and represents a major obstacle to valid causal and statistical inferences; it cannot be removed by randomized experiments and can rarely be detected in either experimental or observational studies. In this paper, we provide complete graphical and algorithmic conditions for recovering conditional probabilities from selection biased data. We also provide graphical conditions for recoverability when unbiased data is available over a subset of the variables. Finally, we provide a graphical condition that generalizes the backdoor criterion and serves to recover causal effects when the data is collected under preferential selection."
aaai,AAAI (Artificial Intelligence),2013,HC-Search: Learning Heuristics and Cost Functions for Structured Prediction,"Janardhan Rao Doppa, Oregon State University|; et al.|Alan Fern, Oregon State University|Prasad Tadepalli, Oregon State University",https://www.semanticscholar.org/paper/73d38c70efb076bed6a218b8290ef748cd738dda,"Structured prediction is the problem of learning a function from structured inputs to structured outputs. Inspired by the recent successes of search-based structured prediction, we introduce a new framework for structured prediction called HC-Search. Given a structured input, the framework uses a search procedure guided by a learned heuristic H to uncover high quality candidate outputs and then uses a separate learned cost function C to select a final prediction among those outputs. We can decompose the regret of the overall approach into the loss due to H not leading to high quality outputs, and the loss due to C not selecting the best among the generated outputs. Guided by this decomposition, we minimize the overall regret in a greedy stagewise manner by first training H to quickly uncover high quality outputs via imitation learning, and then training C to correctly rank the outputs generated via H according to their true losses. Experiments on several benchmark domains show that our approach significantly outperforms the state-of-the-art methods."
aaai,AAAI (Artificial Intelligence),2012,Learning SVM Classifiers with Indefinite Kernels,"Suicheng Gu & Yuhong Guo, Temple University",https://www.semanticscholar.org/paper/535d35b1eb40e7c91af2e54850d565524d07f13d,"Recently, training support vector machines with indefinite kernels has attracted great attention in the machine learning community. In this paper, we tackle this problem by formulating a joint optimization model over SVM classifications and kernel principal component analysis. We first reformulate the kernel principal component analysis as a general kernel transformation framework, and then incorporate it into the SVM classification to formulate a joint optimization model. The proposed model has the advantage of making consistent kernel transformations over training and test samples. It can be used for both binary classification and multiclass classification problems. Our experimental results on both synthetic data sets and real world data sets show the proposed model can significantly outperform related approaches."
aaai,AAAI (Artificial Intelligence),2012,Document Summarization Based on Data Reconstruction,"Zhanying He, Zhejiang University|; et al.|Chun Chen, Zhejiang University|Jiajun Bu, Zhejiang University|Can Wang, Zhejiang University|Lijan Zhang, Zhejiang University|Deng Cai, Zhejiang University|Xiaofei He, Zhejiang University",https://www.semanticscholar.org/paper/d7c58e4f16504500329315e06eeba700c4b7abca,"Document summarization is of great value to many real world applications, such as snippets generation for search results and news headlines generation. Traditionally, document summarization is implemented by extracting sentences that cover the main topics of a document with a minimum redundancy. In this paper, we take a different perspective from data reconstruction and propose a novel framework named Document Summarization based on Data Reconstruction (DSDR). Specifically, our approach generates a summary which consist of those sentences that can best reconstruct the original document. To model the relationship among sentences, we introduce two objective functions: (1) linear reconstruction, which approximates the document by linear combinations of the selected sentences; (2) nonnegative linear reconstruction, which allows only additive, not subtractive, linear combinations. In this framework, the reconstruction error becomes a natural criterion for measuring the quality of the summary. For each objective function, we develop an efficient algorithm to solve the corresponding optimization problem. Extensive experiments on summarization benchmark data sets DUC 2006 and DUC 2007 demonstrate the effectiveness of our proposed approach."
aaai,AAAI (Artificial Intelligence),2011,Complexity of and Algorithms for Borda Manipulation,"Jessica Davies, University of Toronto|; et al.|George Katsirelos, University of Paris-Sud|Nina Narodytska, University of New South Wales|Toby Walsh, National ICT Australia",https://www.semanticscholar.org/paper/08d354d27463922ac5ae02f6f93f7eec98c40dd8,"We prove that it is NP-hard for a coalition of two manipulators to compute how to manipulate the Borda voting rule. This resolves one of the last open problems in the computational complexity of manipulating common voting rules. Because of this NP-hardness, we treat computing a manipulation as an approximation problem where we try to minimize the number of manipulators. Based on ideas from bin packing and multiprocessor scheduling, we propose two new approximation methods to compute manipulations of the Borda rule. Experiments show that these methods significantly outperform the previous best known approximation method. We are able to find optimal manipulations in almost all the randomly generated elections tested. Our results suggest that, whilst computing a manipulation of the Borda rule by a coalition is NP-hard, computational complexity may provide only a weak barrier against manipulation in practice."
aaai,AAAI (Artificial Intelligence),2011,Dynamic Resource Allocation in Conservation Planning,"Daniel Golovin, California Institute of Technology|; et al.|Andreas Krause, ETH Zurich|Beth Gardner, North Carolina State University|Sarah J. Converse, Patuxent Wildlife Research Center|Steve Morey, United States Fish and Wildlife Service",https://www.semanticscholar.org/paper/1f9a83982a9dad2ba9ec991e216d887936c3d40e,"Consider the problem of protecting endangered species by selecting patches of land to be used for conservation purposes. Typically, the availability of patches changes over time, and recommendations must be made dynamically. This is a challenging prototypical example of a sequential optimization problem under uncertainty in computational sustainability. Existing techniques do not scale to problems of realistic size. In this paper, we develop an efficient algorithm for adaptively making recommendations for dynamic conservation planning, and prove that it obtains near-optimal performance. We further evaluate our approach on a detailed reserve design case study of conservation planning for three rare species in the Pacific Northwest of the United States."
aaai,AAAI (Artificial Intelligence),2010,A Novel Transition Based Encoding Scheme for Planning as Satisfiability,"Ruoyun Huang, Washington University in St. Louis|; et al.|Yixin Chen, Washington University in St. Louis|Weixiong Zhang, Washington University in St. Louis",https://www.semanticscholar.org/paper/8d6c0fef0afd9f7654394a8050ae74faaece5bb1,"Planning as satisfiability is a principal approach to planning with many eminent advantages. The existing planning as satisfiability techniques usually use encodings compiled from the STRIPS formalism. We introduce a novel SAT encoding scheme based on the SAS+formalism. It exploits the structural information in the SAS+ formalism, resulting in more compact SAT instances and reducing the number of clauses by up to 50 fold. Our results show that this encoding scheme improves upon the STRIPS-based encoding, in terms of both time and memory efficiency."
aaai,AAAI (Artificial Intelligence),2008,How Good is Almost Perfect?,"Malte Helmert & Gabriele Röger, University of Freiburg",https://www.semanticscholar.org/paper/4ed56a5f8138d158a1089bf7b472642a64647b3e,"Heuristic search using algorithms such as A* and IDA* is the prevalent method for obtaining optimal sequential solutions for classical planning tasks. Theoretical analyses of these classical search algorithms, such as the well-known results of Pohl, Gaschnig and Pearl, suggest that such heuristic search algorithms can obtain better than exponential scaling behaviour, provided that the heuristics are accurate enough. 
 
Here, we show that for a number of common planning benchmark domains, including ones that admit optimal solution in polynomial time, general search algorithms such as A* must necessarily explore an exponential number of search nodes even under the optimistic assumption of almost perfect heuristic estimators, whose heuristic error is bounded by a small additive constant. 
 
Our results shed some light on the comparatively bad performance of optimal heuristic search approaches in ""simple"" planning domains such as GRIPPER. They suggest that in many applications, further improvements in run-time require changes to other parts of the search algorithm than the heuristic estimator."
aaai,AAAI (Artificial Intelligence),2008,Optimal False-Name-Proof Voting Rules with Costly Voting,"Liad Wagman & Vincent Conitzer, Duke University",https://www.semanticscholar.org/paper/098da9f6ad978959c906be83fd5d5b24bacb214b,"One way for agents to reach a joint decision is to vote over the alternatives. In open, anonymous settings such as the Internet, an agent can vote more than once without being detected. A voting rule is false-name-proof if no agent ever benefits from casting additional votes. Previous work has shown that all false-name-proof voting rules are unresponsive to agents' preferences. However, that work implicitly assumes that casting additional votes is costless. In this paper, we consider what happens if there is a cost to casting additional votes. We characterize the optimal (most responsive) false-name-proofwith-costs voting rule for 2 alternatives. In sharp contrast to the costless setting, we prove that as the voting population grows larger, the probability that this rule selects the majority winner converges to 1. We also characterize the optimal group false-name-proof rule for 2 alternatives, which is robust to coalitions of agents sharing the costs of additional votes. Unfortunately, the probability that this rule chooses the majority winner as the voting population grows larger is relatively low. We derive an analogous rule in a setting with 3 alternatives, and provide bounding results and computational approaches for settings with 4 or more alternatives."
aaai,AAAI (Artificial Intelligence),2007,PLOW: A Collaborative Task Learning Agent,"James Allen, Florida Institute for Human and Machine Cognition|; et al.|Nathanael Chambers, Stanford University|George Ferguson, University of Rochester|Lucian Galescu, Florida Institute for Human and Machine Cognition|Hyuckchul Jung, Florida Institute for Human and Machine Cognition|Mary Swift, University of Rochester|William Taysom, Florida Institute for Human and Machine Cognition",https://www.semanticscholar.org/paper/431e61648a59abcd05411503ead56de8aa97906b,"To be effective, an agent that collaborates with humans needs to be able to learn new tasks from humans they work with. This paper describes a system that learns executable task models from a single collaborative learning session consisting of demonstration, explanation and dialogue. To accomplish this, the system integrates a range of AI technologies: deep natural language understanding, knowledge representation and reasoning, dialogue systems, planning/agent-based systems and machine learning. A formal evaluation shows the approach has great promise."
aaai,AAAI (Artificial Intelligence),2007,"Thresholded Rewards: Acting Optimally in Timed, Zero-Sum Games","Colin McMillen & Manuela Veloso, Carnegie Mellon University",https://www.semanticscholar.org/paper/e1f99232103402e134eef9bded15f5d7ec142672,"In timed, zero-sum games, the goal is to maximize the probability of winning, which is not necessarily the same as maximizing our expected reward. We consider cumulative intermediate reward to be the difference between our score and our opponent's score; the ""true"" reward of a win, loss, or tie is determined at the end of a game by applying a threshold function to the cumulative intermediate reward. We introduce thresholded-rewards problems to capture this dependency of the final reward outcome on the cumulative intermediate reward. Thresholded-rewards problems reflect different real-world stochastic planning domains, especially zero-sum games, in which time and score need to be considered. We investigate the application of thresholded rewards to finite-horizon Markov Decision Processes (MDPs). In general, the optimal policy for a thresholded-rewards MDP will be non-stationary, depending on the number of time steps remaining and the cumulative intermediate reward. We introduce an efficient value iteration algorithm that solves thresholded-rewards MDPs exactly, but with running time quadratic on the number of states in the MDP and the length of the time horizon. We investigate a number of heuristic-based techniques that efficiently find approximate solutions for MDPs with large state spaces or long time horizons."
aaai,AAAI (Artificial Intelligence),2006,Model Counting: A New Strategy for Obtaining Good Bounds,"Carla P. Gomes, Cornell University|; et al.|Ashish Sabharwal, Cornell University|Bart Selman, Cornell University",https://www.semanticscholar.org/paper/d1ed22234e6b143b3abfd2a30dbbc6b16eafb67c,"Model counting is the classical problem of computing the number of solutions of a given propositional formula. It vastly generalizes the NP-complete problem of propositional satisfiability, and hence is both highly useful and extremely expensive to solve in practice. We present a new approach to model counting that is based on adding a carefully chosen number of so-called streamlining constraints to the input formula in order to cut down the size of its solution space in a controlled manner. Each of the additional constraints is a randomly chosen XOR or parity constraint on the problem variables, represented either directly or in the standard CNF form. Inspired by a related yet quite different theoretical study of the properties of XOR constraints, we provide a formal proof that with high probability, the number of XOR constraints added in order to bring the formula to the boundary of being unsatisfiable determines with high precision its model count. Experimentally, we demonstrate that this approach can be used to obtain good bounds on the model counts for formulas that are far beyond the reach of exact counting methods. In fact, we obtain the first non-trivial solution counts for very hard, highly structured combinatorial problem instances. Note that unlike other counting techniques, such as Markov Chain Monte Carlo methods, we are able to provide high-confidence guarantees on the quality of the counts obtained."
aaai,AAAI (Artificial Intelligence),2006,Towards an Axiom System for Default Logic,"Gerhard Lakemeyer, RWTH Aachen University|Hector J. Levesque, University of Toronto",https://www.semanticscholar.org/paper/4ee79c2d1868921b30ffda8b8eb2203dba7315fd,"Recently, Lakemeyer and Levesque proposed a logic of only-knowing which precisely captures three forms of nonmonotonic reasoning: Moore's Autoepistemic Logic, Konolige's variant based on moderately grounded expansions, and Reiter's default logic. Defaults have a uniform representation under all three interpretations in the new logic. Moreover, the logic itself is monotonic, that is, nonmonotonic reasoning is cast in terms of validity in the classical sense. While Lakemeyer and Levesque gave a model-theoretic account of their logic, a proof-theoretic characterization remained open. This paper fills that gap for the propositional subset: a sound and complete axiom system in the new logic for all three varieties of default reasoning. We also present formal derivations for some examples of default reasoning. Finally we present evidence that it is unlikely that a complete axiom system exists in the first-order case, even when restricted to the simplest forms of default reasoning."
aaai,AAAI (Artificial Intelligence),2005,The Max K- Armed Bandit: A New Model of Exploration Applied to Search Heuristic Selection,"Vincent A. Cicirello, Drexel University|Stephen F. Smith, Carnegie Mellon University",https://www.semanticscholar.org/paper/53b61519e4fa067036213badca926936c095cd1f,"The multiarmed bandit is often used as an analogy for the tradeoff between exploration and exploitation in search problems. The classic problem involves allocating trials to the arms of a multiarmed slot machine to maximize the expected sum of rewards. We pose a new variation of the multiarmed bandit--the Max K-Armed Bandit--in which trials must be allocated among the arms to maximize the expected best single sample reward of the series of trials. Motivation for the Max K-Armed Bandit is the allocation of restarts among a set of multistart stochastic search algorithms. We present an analysis of this Max K-Armed Bandit showing under certain assumptions that the optimal strategy allocates trials to the observed best arm at a rate increasing double exponentially relative to the other arms. This motivates an exploration strategy that follows a Boltzmann distribution with an exponentially decaying temperature parameter. We compare this exploration policy to policies that allocate trials to the observed best arm at rates faster (and slower) than double exponentially. The results confirm, for two scheduling domains, that the double exponential increase in the rate of allocations to the observed best heuristic outperfonns the other approaches."
aaai,AAAI (Artificial Intelligence),2004,Learning and Inferring Transportation Routines,"Lin Liao, University of Washington|; et al.|Dieter Fox, University of Washington|Henry Kautz, University of Washington",https://www.semanticscholar.org/paper/51c41c33908ebda05c4863474423a36d20c3b8ae,"This paper introduces a hierarchical Markov model that can learn and infer a user's daily movements through an urban community. The model uses multiple levels of abstraction in order to bridge the gap between raw GPS sensor measurements and high level information such as a user's destination and mode of transportation. To achieve efficient inference, we apply Rao-Blackwellized particle filters at multiple levels of the model hierarchy. Locations such as bus stops and parking lots, where the user frequently changes mode of transportation, are learned from GPS data logs without manual labeling of training data. We experimentally demonstrate how to accurately detect novel behavior or user errors (e.g. taking a wrong bus) by explicitly modeling activities in the context of the user's historical data. Finally, we discuss an application called ''Opportunity Knocks'' that employs our techniques to help cognitively-impaired people use public transportation safely."
aaai,AAAI (Artificial Intelligence),2002,On Computing All Abductive Explanations,"Thomas Eiter, TU Wien|Kazuhisa Makino, Osaka University",https://www.semanticscholar.org/paper/2b8b6a760d8482786007f5b269a9468a6159c02e,"We consider the computation of all respectively a polynomial subset of the explanations of an abductive query from a Horn theory, and pay particular attention to whether the query is a positive or negative letter, the explanation is based on literals from an assumption set, and the Horn theory is represented in terms of formulas or characteristic models. We derive tractability results, one of which refutes a conjecture by Selman and Levesque, as well as intractability results, and furthermore also semi-tractability results in terms of solvability in quasi-polynomial time. Our results complement previous results in the literature, and elucidate the computational complexity of generating the set of explanations."
aaai,AAAI (Artificial Intelligence),2000,Local Search Characteristics of Incomplete SAT Procedures,"Dale Schuurmans & Finnegan Southey, University of Waterloo",https://www.aaai.org/Library/AAAI/2000/aaai00-046.php,AAAI advances the understanding of the mechanisms underlying thought and intelligent behavior and their embodiment in machines.
aaai,AAAI (Artificial Intelligence),2000,The Game of Hex: An Automatic Theorem-Proving Approach to Game Programming,"Vadim V. Anshelevich, Vanshel Consulting",https://www.semanticscholar.org/paper/7c7144e79a391c1c3577f1674874b469aa7072da,"The game of Hex is a two-player game with simple rules, a deep underlying mathematical beauty, and a strategic complexity comparable to that of Chess and Go. The massive game-tree search techniques developed mostly for Chess, and successfully used for Checkers, Othello, and a number of other games, become less useful for games with large branching factors like Go and Hex. We offer a new approach, which results in superior playing strength. This approach emphasizes deep analysis of relatively few game positions. In order to reach this goal, we develop an automatic theorem proving technique for topological analysis of Hex positions. We also discuss in detail an idea of modeling Hex positions with electrical resistor circuits. We explain how this approach is implemented in Hexy - the strongest known Hex-playing computer program, able to compete with best human players."
aaai,AAAI (Artificial Intelligence),2000,Automatic Invention of Integer Sequences,"Simon Colton, University of Edinburgh|; et al.|Alan Bundy, University of Edinburgh|Toby Walsh, University of York",https://www.aaai.org/Library/AAAI/2000/aaai00-085.php,AAAI advances the understanding of the mechanisms underlying thought and intelligent behavior and their embodiment in machines.
aaai,AAAI (Artificial Intelligence),1999,PROVERB: The Probabilistic Cruciverbalist,"Greg A. Keim, Duke University|; et al.|Noam M. Shazeer, Duke University|Michael L. Littman, Duke University|Sushant Agarwal, Duke University|Catherine M. Cheves, Duke University|Joseph Fitzgerald, Duke University|Jason Grosland, Duke University|Fan Jiang, Duke University|Shannon Pollard, Duke University|Karl Weinmeister, Duke University",https://www.semanticscholar.org/paper/bd417d347a18e52598b3ccafcf5d8031f45c0f41,"We attacked the problem of solving crossword puzzles by computer: given a set of clues and a crossword grid, try to maximize the number of words correctly filled in. In our system, ""expert modules"" specialize in solving specific types of clues, drawing on ideas from information retrieval, database search, and machine learning. Each expert module generates a (possibly empty) candidate list for each clue, and the lists are merged together and placed into the grid by a centralized solver. We used a probabilistic representation throughout the system as a common interchange language between subsystems and to drive the search for an optimal solution. PROVERB, the complete system, averages 95.3% words correct and 98.1 % letters correct in under 15 minutes per puzzle on a sample of 370 puzzles taken from the New York Times and several other puzzle sources. This corresponds to missing roughly 3 words or 4 letters on a daily 15 × 15 puzzle, making PROVERB a better-than-average cruciverbalist (crossword solver)."
aaai,AAAI (Artificial Intelligence),1998,Learning Evaluation Functions for Global Optimization and Boolean Satisfiability,"Justin A. Boyan & Andrew W. Moore, Carnegie Mellon University",https://www.semanticscholar.org/paper/bddddac9b0e4c3a3a451f797aa2e8641be79bc60,"This paper describes STAGE, a learning approach to automatically improving search performance on optimization problems. STAGE learns an evaluation function which predicts the outcome of a local search algorithm, such as hillclimbing or WALKSAT, as a function of state features along its search trajectories. The learned evaluation function is used to bias future search trajectories toward better optima. We present positive results on six large-scale optimization domains."
aaai,AAAI (Artificial Intelligence),1998,Acceleration Methods for Numeric CSPs,"Yahia Lebbah & Olivier Lhomme, École des Mines de Nantes",https://www.semanticscholar.org/paper/469d7c1c9f4a4ab8b57774a7f19b1df563d2689b,"This paper introduces a new way of accelerating the convergence of numeric CSP filtering algorithms, through the use of extrapolation methotis. Extrapolation methods are used in numerical analysis to accelerate the convergence of real number sequences. We will show how to use them for solving numeric sPs, leading to drastic improvement in efficiency."
aaai,AAAI (Artificial Intelligence),1998,The Interactive Museum Tour-Guide Robot,"Wolfram Burgard, University of Bonn|; et al.|Armin B. Cremers, University of Bonn|Dieter Fox, University of Bonn|Dirk Hähnel, University of Bonn|Gerhard Lakemeyer, RWTH Aachen University|Dirk Schulz, University of Bonn|Walter Steiner, University of Bonn|Sebastian Thrun, Carnegie Mellon University",https://www.semanticscholar.org/paper/198a5389d395f7a2aea1523e8d54d8c0f72ac366,"This paper describes the software architecture of an autonomous tour-guideltutor robot. This robot was recently deployed in the ""Deutsches Museum Bonn,"" were it guided hundreds of visitors through the museum during a six-day deployment period. The robot's control software integrates low-level probabilistic reasoning with high-level problem solving embedded in first order logic. A collection of software innovations, described in this paper, enabled the robot to navigate at high speeds through dense crowds, while reliably avoiding collisions with obstacles--some of which could not even be perceived. Also described in this paper is a user interface tailored towards non-expert users, which was essential for the robot's success in the museum. Based on these experiences, this paper argues that time is ripe for the development of AI-based commercial service robots that assist people in everyday life."
aaai,AAAI (Artificial Intelligence),1997,A Practical Algorithm for Finding Optimal Triangulations,"Krill Shoikhet & Dan Geiger, Technion – Israel Institute of Technology",https://www.semanticscholar.org/paper/7c1f5a78fdb24266467ee95dbae4f2447a0add17,An algorithm called QUICKTREE is developed for finding a triangulation T of a given undirected graph G such that the size of T's maximal clique is minimum and such that no other triangulation of G is a subgraph of T. We have tested QUICKTREE on graphs of up to 100 nodes for which the maximum clique in an optimal triangulation is of size 11. This is the first algorithm that can optimally triangulate graphs of such size in a reasonable time frame. This algorithm is useful for constraint satisfaction problems and for Bayesian inference through the clique tree inference algorithm.
aaai,AAAI (Artificial Intelligence),1997,Statistical Parsing with a Context-Free Grammar and Word Statistics,"Eugene Charniak, Brown University",https://www.semanticscholar.org/paper/2a5e619f2c5f4220438b1357e596db5b1578398d,"We describe a parsing system based upon a language model for English that is, in turn, based upon assigning probabilities to possible parses for a sentence. This model is used in a parsing system by finding the parse for the sentence with the highest probability. This system outperforms previous schemes. As this is the third in a series of parsers by different authors that are similar enough to invite detailed comparisons but different enough to give rise to different levels of performance, we also report on some experiments designed to identify what aspects of these systems best explain their relative performance."
aaai,AAAI (Artificial Intelligence),1997,Building Concept Representations from Reusable Components,"Peter Clark, Boeing|Bruce Porter, University of Texas at Austin",https://www.semanticscholar.org/paper/00ee2e3f19be3be3a0303ac08ed5fc60e0d38c69,"Our goal is to build knowledge-based systems capable of answering a wide variety of questions, including questions that are unanticipated when the knowledge base is built. For systems to achieve this level of competence and generality, they require the ability to dynamically construct new concept representations, and to do so in response to the questions arLd tasks posed to them. Our approach to meeting this requirement is to build knowledge bases of generalized, representational components, and to develop methods for automatically composing components on demand. This work extends the normal inheritance approach used in frame-based systems, and imports ideas from several different areas of AI, in particular compositional modeling, terminological reasoning, and ontological engineering. The contribution of this work is a novel integration of these methods that improves the efficiency of building knowledge bases and the robustness of using them."
aaai,AAAI (Artificial Intelligence),1997,Fast Context Switching in Real-Time Propositional Reasoning,"P. Pandurang Nayak & Brian C. Williams, Ames Research Center",https://www.semanticscholar.org/paper/0bfa40fe50255a42cac34c3223871fec2239f4e4,"The trend to increasingly capable and affordable control processors has generated an explosion of embedded real-time gadgets that serve almost every function imaginable. The daunting task of programming these gadgets is greatly alleviated with real-time deductive engines that perform all execution and monitoring functions from a single core model. Fast response times are achieved using an incremental propositional deductive database (an LTMS). Ideally the cost of an LTMS's incremental update should be linear in the number of labels that change between successive contexts. Unfortunately an LTMS can expend a significant percentage of its time working on labels that remain constant between contexts. This is caused by the LTMS's conservative approach: a context switch first removes all consequences of deleted clauses, whether or not those consequences hold in the new context. This paper presents a more aggressive incremental TMS, called the ITMS, that avoids processing a significant number of these consequences that are unchanged. Our empirical evaluation for spacecraft control shows that the overhead of processing unchanged consequences can be reduced by a factor of seven."
aaai,AAAI (Artificial Intelligence),1996,A Novel Application of Theory Refinement to Student Modeling,"Paul T. Baffes, ScicomP|Raymond J. Mooney, University of Texas at Austin",https://www.semanticscholar.org/paper/466b65ec9f13864e0804362aa64d352ba33a117f,Theory refinement systems developed in machine learning automatically modify a knowledge base to render it consistent with a set of classified training examples. We illustrate a novel application of these techniques to the problem of constructing a student model for an intelligent tutoring system (ITS). Our approach is implemented in an ITS authoring system called ASSERT which uses theory refinement to introduce errors into an initially correct knowledge base so that it models incorrect student behavior. The efficacy of the approach has been demonstrated by evaluating a tutor developed with ASSERT with 75 students tested on a classification task covering concepts from an introductory course on the C++ programmm. g Ia nguage. The system produced reasonably accurate models and students who received feedback based on these models performed significantly better on a post test than students who received simple reteaching.
aaai,AAAI (Artificial Intelligence),1996,Verification of Knowledge Bases Based on Containment Checking,"Alon Y. Levy, AT&T Laboratories|Marie-Christine Rousset, University of Paris-Sud",https://www.semanticscholar.org/paper/5c8f5e53e0ba6c7034c16c678143930fd76f89f5,"Building complex knowledge based applications requires encoding large amounts of domain knowledge. After acquiring knowledge from domain experts, much of the effort in building a knowledge base goes into verifying that the knowledge is encoded correctly. We consider the problem of verifying hybrid knowledge bases that contain both Horn rules and a terminology in a description logic. Our approach to the verification problem is based on showing a close relationship to the problem of query containment. Our first contribution, based on this relationship, is presenting a thorough analysis of the decidability and complexity of the verification problem, for knowledge bases containing recursive rules and the interpreted predicates =, ≤, < and ≠. Second, we show that important new classes of constraints on correct inputs and outputs can be expressed in a hybrid setting, in which a description logic class hierarchy is also considered, and we present the first complete algorithm for verifying such hybrid knowledge bases."
aaai,AAAI (Artificial Intelligence),1996,"Pushing the Envelope: Planning, Propositional Logic, and Stochastic Search","Henry Kautz & Bart Selman, AT&T Laboratories",https://www.semanticscholar.org/paper/141c77b1d82bcae04a293c972cea02502e181dba,"Planning is a notoriously hard combinatorial search problem. In many interesting domains, current planning algorithms fail to scale up gracefully. By combining a general, stochastic search algorithm and appropriate problem encodings based on propositional logic, we are able to solve hard planning problems many times faster than the best current planning systems. Although stochastic methods have been shown to be very effective on a wide range of scheduling problems, this is the first demonstration of its power on truly challenging classical planning instances. This work also provides a new perspective on representational issues in planning."
ijcai,IJCAI (Artificial Intelligence),2018,Reasoning about Consensus when Opinions Diffuse through Majority Dynamics,"Vincenzo Auletta, University of Salerno|; et al.|Diodato Ferraioli, University of Salerno|Gianluigi Greco, University of Calabria",https://www.semanticscholar.org/paper/cae7b76c04032d5a3e45bdf6c2f8f4d928a8d038,"Opinion diffusion is studied on social graphs where agents hold binary opinions and where social pressure leads them to conform to the opinion manifested by the majority of their neighbors. Within this setting, questions related to whether a minority/majority can spread the opinion it supports to all the other agents are considered. It is shown that, no matter of the underlying graph, there is always a group formed by a half of the agents that can annihilate the opposite opinion. Instead, the influence power of minorities depends on certain features of the given graph, which are NP-hard to be identified. Deciding whether the two opinions can coexist in some stable configuration is NP-hard, too."
ijcai,IJCAI (Artificial Intelligence),2018,What game are we playing? End-to-end learning in normal and extensive form games,"Chun Kai Ling, Carnegie Mellon University|; et al.|Fei Fang, Carnegie Mellon University|J. Zico Kolter, Carnegie Mellon University",https://www.semanticscholar.org/paper/8d859af05b1f50708ff83aa8a5623c4cbc45d490,"Although recent work in AI has made great progress in solving large, zero-sum, extensive-form games, the underlying assumption in most past work is that the parameters of the game itself are known to the agents. This paper deals with the relatively under-explored but equally important ""inverse"" setting, where the parameters of the underlying game are not known to all agents, but must be learned through observations. We propose a differentiable, end-to-end learning framework for addressing this task. In particular, we consider a regularized version of the game, equivalent to a particular form of quantal response equilibrium, and develop 1) a primal-dual Newton method for finding such equilibrium points in both normal and extensive form games; and 2) a backpropagation method that lets us analytically compute gradients of all relevant game parameters through the solution itself. This ultimately lets us learn the game by training in an end-to-end fashion, effectively by integrating a ""differentiable game solver"" into the loop of larger deep network architectures. We demonstrate the effectiveness of the learning method in several settings including poker and security game tasks."
ijcai,IJCAI (Artificial Intelligence),2018,From Conjunctive Queries to Instance Queries in Ontology-Mediated Querying,"Cristina Feier, University of Bremen|; et al.|Carsten Lutz, University of Bremen|Frank Wolte, University of Liverpool",https://www.semanticscholar.org/paper/e36440509511e82d646407ec94a8b859ccdf3494,"© 2018 CEUR-WS. All rights reserved. We consider the rewritability of ontology-mediated queries (OMQs) based on UCQs into OMQs based on (certain kinds of) SPARQL queries. Our focus is on ALCI as a paradigmatic expressive DL. For rewritability into SPARQL queries that are unions of basic graph patterns, we show that the existence of a rewriting is decidable, based on a suitable characterization; for unary OMQs, this coincides with rewritability into instance queries. For SPARQL queries that additionally admit projection, we make some interesting first observations. In particular, we show that whenever there is a rewriting, then there is one that uses the same TBox as the original OMQ and only ALCI concepts from a certain finite class of such concepts. We also observe that if the TBox of the original OMQ falls into Horn-ALCI, then a rewriting always exists."
ijcai,IJCAI (Artificial Intelligence),2018,R-SVM+: Robust Learning with Privileged Information,"Xue Li, Wuhan University|; et al.|Bo Du, Wuhan University|Chang Xu, University of Sydney|Yipeng Zhang, Wuhan University|Lefei Zhang, Wuhan University|Dacheng Tao, University of Sydney",https://www.semanticscholar.org/paper/8b4048325bfe14a2dc11dea88626e35b7cb5e59f,"In practice, the circumstance that training and test data are clean is not always satisfied. The performance of existing methods in the learning using privileged information (LUPI) paradigm may be seriously challenged, due to the lack of clear strategies to address potential noises in the data. This paper proposes a novel Robust SVM+ (RSVM+) algorithm based on a rigorous theoretical analysis. Under the SVM+ framework in the LUPI paradigm, we study the lower bound of perturbations of both example feature data and privileged feature data, which will mislead the model to make wrong decisions. By maximizing the lower bound, tolerance of the learned model over perturbations will be increased. Accordingly, a novel regularization function is introduced to upgrade a variant form of SVM+. The objective function of RSVM+ is transformed into a quadratic programming problem, which can be efficiently optimized using off-the-shelf solvers. Experiments on realworld datasets demonstrate the necessity of studying robust SVM+ and the effectiveness of the proposed algorithm."
ijcai,IJCAI (Artificial Intelligence),2018,Commonsense Knowledge Aware Conversation Generation with Graph Attention,"Hao Zhou, Tsinghua University|; et al.|Tom Young, Beijing Institute of Technology|Minlie Huang, Tsinghua University|Haizhou Zhao, Sogou|Jingfang Xu, Sogou|Xiaoyan Zhu, Tsinghua University",https://www.semanticscholar.org/paper/c2ae7c861d4b8b310d1318e1fdb6135b1739801f,"Commonsense knowledge is vital to many natural language processing tasks. In this paper, we present a novel open-domain conversation generation model to demonstrate how large-scale commonsense knowledge can facilitate language understanding and generation. Given a user post, the model retrieves relevant knowledge graphs from a knowledge base and then encodes the graphs with a static graph attention mechanism, which augments the semantic information of the post and thus supports better understanding of the post. Then, during word generation, the model attentively reads the retrieved knowledge graphs and the knowledge triples within each graph to facilitate better generation through a dynamic graph attention mechanism. This is the first attempt that uses large-scale commonsense knowledge in conversation generation. Furthermore, unlike existing models that use knowledge triples (entities) separately and independently, our model treats each knowledge graph as a whole, which encodes more structured, connected semantic information in the graphs. Experiments show that the proposed model can generate more appropriate and informative responses than stateof-the-art baselines."
ijcai,IJCAI (Artificial Intelligence),2018,A Degeneracy Framework for Graph Similarity,"Giannis Nikolentzos, École Polytechnique|; et al.|Polykarpos Meladianos, Athens University of Economics and Business|Stratis Limnios, École Polytechnique|Michalis Vazirgiannis, École Polytechnique",https://www.semanticscholar.org/paper/90678e09651954ace70fe645c5cc8d8f0d8acdb7,"The problem of accurately measuring the similarity between graphs is at the core of many applications in a variety of disciplines. Most existing methods for graph similarity focus either on local or on global properties of graphs. However, even if graphs seem very similar from a local or a global perspective, they may exhibit different structure at different scales. In this paper, we present a general framework for graph similarity which takes into account structure at multiple different scales. The proposed framework capitalizes on the wellknown k-core decomposition of graphs in order to build a hierarchy of nested subgraphs. We apply the framework to derive variants of four graph kernels, namely graphlet kernel, shortest-path kernel, Weisfeiler-Lehman subtree kernel, and pyramid match graph kernel. The framework is not limited to graph kernels, but can be applied to any graph comparison algorithm. The proposed framework is evaluated on several benchmark datasets for graph classification. In most cases, the corebased kernels achieve significant improvements in terms of classification accuracy over the base kernels, while their time complexity remains very attractive."
ijcai,IJCAI (Artificial Intelligence),2018,SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks,"Ke Wang & Xiaojun Wan, Peking University",https://www.semanticscholar.org/paper/55caf5154cd558c355d7191daa565ebebb8336e1,"Generating texts of different sentiment labels is getting more and more attention in the area of natural language generation. Recently, Generative Adversarial Net (GAN) has shown promising results in text generation. However, the texts generated by GAN usually suffer from the problems of poor quality, lack of diversity and mode collapse. In this paper, we propose a novel framework SentiGAN, which has multiple generators and one multi-class discriminator, to address the above problems. In our framework, multiple generators are trained simultaneously, aiming at generating texts of different sentiment labels without supervision. We propose a penalty based objective in the generators to force each of them to generate diversified examples of a specific sentiment label. Moreover, the use of multiple generators and one multi-class discriminator can make each generator focus on generating its own examples of a specific sentiment label accurately. Experimental results on four datasets demonstrate that our model consistently outperforms several state-of-the-art text generation methods in the sentiment accuracy and quality of generated texts."
ijcai,IJCAI (Artificial Intelligence),2017,Foundations of Declarative Data Analysis Using Limit Datalog Programs,"Mark Kaminski, University of Oxford|; et al.|Bernardo Cuenca Grau, University of Oxford|Egor V. Kostylev, University of Oxford|Boris Motik, University of Oxford|Ian Horrocks, University of Oxford",https://www.semanticscholar.org/paper/76591b19c0f1e55f0c58cb9aea1f53881df7d730,"Motivated by applications in declarative data analysis, we study $\mathit{Datalog}_{\mathbb{Z}}$---an extension of positive Datalog with arithmetic functions over integers. This language is known to be undecidable, so we propose two fragments. In $\mathit{limit}~\mathit{Datalog}_{\mathbb{Z}}$ predicates are axiomatised to keep minimal/maximal numeric values, allowing us to show that fact entailment is coNExpTime-complete in combined, and coNP-complete in data complexity. Moreover, an additional $\mathit{stability}$ requirement causes the complexity to drop to ExpTime and PTime, respectively. Finally, we show that stable $\mathit{Datalog}_{\mathbb{Z}}$ can express many useful data analysis tasks, and so our results provide a sound foundation for the development of advanced information systems."
ijcai,IJCAI (Artificial Intelligence),2016,Hierarchical Finite State Controllers for Generalized Planning,"Javier Segovia, Pompeu Fabra University|; et al.|Sergio Jiménez, Pompeu Fabra University|Anders Jonsson, Pompeu Fabra University",https://www.semanticscholar.org/paper/3a7046fd62c05212fb2f9662f0845476dd10d516,"Finite State Controllers (FSCs) are an effective way to represent sequential plans compactly. By imposing appropriate conditions on transitions, FSCs can also represent generalized plans that solve a range of planning problems from a given domain. In this paper we introduce the concept of hierarchical FSCs for planning by allowing controllers to call other controllers. We show that hierarchical FSCs can represent generalized plans more compactly than individual FSCs. Moreover, our call mechanism makes it possible to generate hierarchical FSCs in a modular fashion, or even to apply recursion. We also introduce a compilation that enables a classical planner to generate hierarchical FSCs that solve challenging generalized planning problems. The compilation takes as input a set of planning problems from a given domain and outputs a single classical planning problem, whose solution corresponds to a hierarchical FSC."
ijcai,IJCAI (Artificial Intelligence),2015,Recursive Decomposition for Nonconvex Optimization,"Abram L. Friesen & Pedro Domingos, University of Washington",https://www.semanticscholar.org/paper/bab734677d463ef5e48db7b4786e6323b885fbaf,"Continuous optimization is an important problem in many areas of AI, including vision, robotics, probabilistic inference, and machine learning. Unfortunately, most real-world optimization problems are nonconvex, causing standard convex techniques to find only local optima, even with extensions like random restarts and simulated annealing. We observe that, in many cases, the local modes of the objective function have combinatorial structure, and thus ideas from combinatorial optimization can be brought to bear. Based on this, we propose a problem-decomposition approach to nonconvex optimization. Similarly to DPLL-style SAT solvers and recursive conditioning in probabilistic inference, our algorithm, RDIS, recursively sets variables so as to simplify and decompose the objective function into approximately independent subfunctions, until the remaining functions are simple enough to be optimized by standard techniques like gradient descent. The variables to set are chosen by graph partitioning, ensuring decomposition whenever possible. We show analytically that RDIS can solve a broad class of nonconvex optimization problems exponentially faster than gradient descent with random restarts. Experimentally, RDIS outperforms standard techniques on problems like structure from motion and protein folding."
ijcai,IJCAI (Artificial Intelligence),2015,Bayesian Active Learning for Posterior Estimation,"Kirthevasan Kandasamy, Carnegie Mellon University|; et al.|Jeff Schneider, Carnegie Mellon University|Barnabas Poczos, Carnegie Mellon University",https://www.semanticscholar.org/paper/8904ed373f87a9f908b2789da303aa14f188e164,This paper studies active posterior estimation in a Bayesian setting when the likelihood is expensive to evaluate. Existing techniques for posterior estimation are based on generating samples representative of the posterior. Such methods do not consider efficiency in terms of likelihood evaluations. In order to be query efficient we treat posterior estimation in an active regression framework. We propose two myopic query strategies to choose where to evaluate the likelihood and implement them using Gaussian processes. Via experiments on a series of synthetic and real examples we demonstrate that our approach is significantly more query efficient than existing techniques and other heuristics for posterior estimation.
ijcai,IJCAI (Artificial Intelligence),2013,Flexibility and Decoupling in the Simple Temporal Problem,"Michel Wilson, Delft University of Technology|; et al.|Tomas Klos, Delft University of Technology|Cees Witteveen, Delft University of Technology|Bob Huisman, Delft University of Technology",https://www.semanticscholar.org/paper/c614441abecf5a20531585208bf2194bd1b8fbcd,"In this paper we concentrate on finding a suitable metric to determine the flexibility of a Simple Temporal Problem (STP). After reviewing some flexibility metrics that have been proposed, we conclude that these metrics fail to capture the correlation between events specified in the STP, resulting in an overestimation of the available flexibility in the system. We propose to use an intuitively more acceptable flexibility metric based upon uncorrelated time-intervals for the allowed starting times of events in an STP. This metric is shown to be computable in low-polynomial time. As a byproduct of the flexibility computation, we get a decomposition of the STN almost for free: for every possible k-partitioning of the event space, a decomposition can be computed in O(k)-time. Even more importantly, we show that contrary to popular belief, such a decomposition does not affect the flexibility of the original STP."
ijcai,IJCAI (Artificial Intelligence),2013,Bayesian Optimization in High Dimensions via Random Embeddings,"Ziyu Wang, University of British Columbia|; et al.|Masrour Zoghi, University of Amsterdam|Frank Hutter, Freiberg University of Mining and Technology|David Matheson, University of British Columbia|Nando de Freitas, University of British Columbia",https://www.semanticscholar.org/paper/75a0a299e4bbcd1123e9000766ddaad13ec8ae10,"Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple and applies to domains with both categorical and continuous variables. The experiments demonstrate that REMBO can effectively solve high-dimensional problems, including automatic parameter configuration of a popular mixed integer linear programming solver."
ijcai,IJCAI (Artificial Intelligence),2011,Nested Rollout Policy Adaptation for Monte Carlo Tree Search,"Christopher D. Rosin, Parity Computing",https://www.semanticscholar.org/paper/64004549770b2dad4478331c8555ec25e9589ad1,"Monte Carlo tree search (MCTS) methods have had recent success in games, planning, and optimization. MCTS uses results from rollouts to guide search; a rollout is a path that descends the tree with a randomized decision at each ply until reaching a leaf. MCTS results can be strongly influenced by the choice of appropriate policy to bias the rollouts. Most previous work on MCTS uses static uniform random or domain-specific policies. We describe a new MCTS method that dynamically adapts the rollout policy during search, in deterministic optimization problems. Our starting point is Cazenave's original Nested Monte Carlo Search (NMCS), but rather than navigating the tree directly we instead use gradient ascent on the rollout policy at each level of the nested search. We benchmark this new Nested Rollout Policy Adaptation (NRPA) algorithm and examine its behavior. Our test problems are instances of Crossword Puzzle Construction and Morpion Solitaire. Over moderate time scales NRPA can substantially improve search efficiency compared to NMCS, and over longer time scales NRPA improves upon all previous published solutions for the test problems. Results include a new Morpion Solitaire solution that improves upon the previous human-generated record that had stood for over 30 years."
ijcai,IJCAI (Artificial Intelligence),2011,On the Decidability of Connectedness Constraints in 2D and 3D Euclidean Spaces,"Roman Kontchakov, Birkbeck College London|; et al.|Yavor Nenov, University of Manchester|Ian Pratt-Hartmann, University of Manchester|Michael Zakharyaschev, Birkbeck College London",https://www.semanticscholar.org/paper/8689badbd236924700dc8f746f2c8044783157c7,"We investigate (quantifier-free) spatial constraint languages with equality, contact and connectedness predicates, as well as Boolean operations on regions, interpreted over low-dimensional Euclidean spaces. We show that the complexity of reasoning varies dramatically depending on the dimension of the space and on the type of regions considered. For example, the logic with the interior-connectedness predicate (and without contact) is undecidable over polygons or regular closed sets in R2, EXPTIME-complete over polyhedra in R3, and NP-complete over regular closed sets in R3."
ijcai,IJCAI (Artificial Intelligence),2011,Unweighted Coalitional Manipulation Under the Borda Rule is NP-Hard,"Nadja Betzler, Technical University of Berlin|; et al.|Rolf Niedermeier, Technical University of Berlin|Gerhard J. Woeginger, Eindhoven University of Technology",https://www.semanticscholar.org/paper/b45ca033fb083982ade18fd461a9c590b5b46e23,"The Borda voting rule is a positional scoring rule where, for m candidates, for every vote the first candidate receives m- 1 points, the second m- 2 points and so on. A Borda winner is a candidate with highest total score. It has been a prominent open problem to determine the computational complexity of UNWEIGHTED COALITIONAL MANIPULATION UNDER BORDA: Can one add a certain number of additional votes (called manipulators) to an election such that a distinguished candidate becomes a winner? We settle this open problem by showing NP-hardness even for two manipulators and three input votes. Moreover, we discuss extensions and limitations of this hardness result."
ijcai,IJCAI (Artificial Intelligence),2009,Consequence-Driven Reasoning for Horn SHIQ Ontologies,"Yevgeny Kazakov, University of Oxford",https://www.semanticscholar.org/paper/85517744a454392e070d59b4e7463400a8e114e7,"We present a novel reasoning procedure for Horn SHIQ ontologies--SHIQ ontologies that can be translated to the Horn fragment of first-order logic. In contrast to traditional reasoning procedures for ontologies, our procedure does not build models or model representations, but works by deriving new consequent axioms. The procedure is closely related to the so-called completion-based procedure for EL++ ontologies, and can be regarded as an extension thereof. In fact, our procedure is theoretically optimal for Horn SHIQ ontologies as well as for the common fragment of EL++ and SHIQ. 
 
A preliminary empirical evaluation of our procedure on large medical ontologies demonstrates a dramatic improvement over existing ontology reasoners. Specifically, our implementation allows the classification of the largest available OWL version of Galen. To the best of our knowledge no other reasoner is able to classify this ontology."
ijcai,IJCAI (Artificial Intelligence),2009,Learning Conditional Preference Networks with Queries,"Frederic Koriche, Montpellier 2 University|Bruno Zanuttini, Université de Caen Basse-Normandie",https://www.semanticscholar.org/paper/6a9b1fa83d769d0da1120741a6c0c0829f2cdbd2,"We investigate the problem of eliciting CP-nets in the well-known model of exact learning with equivalence and membership queries. The goal is to identify a preference ordering with a binary-valued CP-net by guiding the user through a sequence of queries. Each example is a dominance test on some pair of outcomes. In this setting, we show that acyclic CP-nets are not learnable with equivalence queries alone, while they are learnable with the help of membership queries if the supplied examples are restricted to swaps. A similar property holds for tree CP-nets with arbitrary examples. In fact, membership queries allow us to provide attribute-efficient algorithms for which the query complexity is only logarithmic in the number of attributes. Such results highlight the utility of this model for eliciting CP-nets in large multi-attribute domains."
ijcai,IJCAI (Artificial Intelligence),2007,Building Structure into Local Search for SAT,"Duc Nghia Pham, Griffith University|; et al.|John Thornton, Griffith University|Abdul Sattar, Griffith University",https://www.semanticscholar.org/paper/59e37114b2dc188d1bdab91e1cb25c09f65706a5,"Local search procedures for solving satisfiability problems have attracted considerable attention since the development of GSAT in 1992. However, recentwork indicates that for many real-world problems, complete search methods have the advantage, because modern heuristics are able to effectively exploit problem structure. Indeed, to develop a local search technique that can effectively deal with variable dependencies has been an open challenge since 1997. 
 
In this paper we show that local search techniques can effectively exploit information about problem structure producing significant improvements in performance on structured problem instances. Building on the earlier work of Ostrowski et al. we describe how information about variable dependencies can be built into a local search, so that only independent variables are considered for flipping. The cost effect of a flip is then dynamically calculated using a dependency lattice that models dependent variables using gates (specifically and, or and equivalence gates). The experimental study on hard structured benchmark problems demonstrates that our new approach significantly outperforms the previously reported best local search techniques."
ijcai,IJCAI (Artificial Intelligence),2007,Automated Heart Wall Motion Abnormality Detection From Ultrasound Images using Bayesian Networks,"Maleeha Qazi, Siemens Medical Solutions|; et al.|Glenn Fung, Siemens Medical Solutions|Sriram Krishnan, Siemens Medical Solutions|Romer Rosales, Siemens Medical Solutions|Harald Steck, Siemens Medical Solutions|R. Bharat Rao, Siemens Medical Solutions|Don Polderman, Erasmus MC|Dhanalakshmi Chandrasekaran, No Affiliation",https://www.semanticscholar.org/paper/88f26b718d318aa78eb80119c3e2b766636fcb92,"Coronary Heart Disease can be diagnosed by measuring and scoring regional motion of the heart wall in ultrasound images of the left ventricle (LV) of the heart. We describe a completely automated and robust technique that detects diseased hearts based on detection and automatic tracking of the endocardium and epicardium of the LV. The local wall regions and the entire heart are then classified as normal or abnormal based on the regional and global LV wall motion. In order to leverage structural information about the heart we applied Bayesian Networks to this problem, and learned the relations among the wall regions off of the data using a structure learning algorithm. We checked the validity of the obtained structure using anatomical knowledge of the heart and medical rules as described by doctors. The resultant Bayesian Network classifier depends only on a small subset of numerical features extracted from dual-contours tracked through time and selected using a filter-based approach. Our numerical results confirm that our system is robust and accurate on echocardiograms collected in routine clinical practice at one hospital; our system is built to be used in real-time."
ijcai,IJCAI (Artificial Intelligence),2007,Performance Analysis of Online Anticipatory Algorithms for Large Multistage Stochastic Integer Pro...,"Luc Mercier & Pascal Van Hentenryck, Brown University",https://www.semanticscholar.org/paper/0557a87dbfe64d4f93b8352c89b1336a78d4b895,"Despite significant algorithmic advances in recent years, finding optimal policies for large-scale, multistage stochastic combinatorial optimization problems remains far beyond the reach of existing methods. This paper studies a complementary approach, online anticipatory algorithms, that make decisions at each step by solving the anticipatory relaxation for a polynomial number of scenarios. Online anticipatory algorithms have exhibited surprisingly good results on a variety of applications and this paper aims at understanding their success. In particular, the paper derives sufficient conditions under which online anticipatory algorithms achieve good expected utility and studies the various types of errors arising in the algorithms including the anticipativity and sampling errors. The sampling error is shown to be negligible with a logarithmic number of scenarios. The anticipativity error is harder to bound and is shown to be low, both theoretically and experimentally, for the existing applications."
ijcai,IJCAI (Artificial Intelligence),2005,Solving Checkers,"Jonathan Schaeffer, University of Alberta|; et al.|Yngvi Bjornsson, University of Alberta|Neil Burch, University of Alberta|Akihiro Kishimoto, University of Alberta|Martin Muller, University of Alberta|Robert Lake, University of Alberta|Paul Lu, University of Alberta|Steve Sutphen, University of Alberta",https://www.semanticscholar.org/paper/ff5e8efe430d079f94b22771d26ae639333ea1ad,"AI has had notable success in building high-performance game-playing programs to complete against the best human players. However, the availability of fast and plentiful machines with large memories and disks creates the possibility of solving a game. This has been done before for simple or relatively small games. In this paper, we present new ideas and algorithms for solving the game of checkers. Checkers is a popular game of skill with a search space of 1020 possible positions. This paper reports on our first result. One of the most challenging checkers openings has been solved-the White Doctor opening is a draw. Solving roughly 50 more openings will result in the game-theoretic value of checkers being determined."
ijcai,IJCAI (Artificial Intelligence),2005,Learning Coordination Classifiers,"Yuhong Guo, University of Alberta|; et al.|Russell Greiner, University of Alberta|Dale Schuurmans, University of Alberta",https://www.semanticscholar.org/paper/400b8ce909d235f61e96f595a21f6487b91b3715,"We present a new approach to ensemble classification that requires learning only a single base classifier. The idea is to learn a classifier that simultaneously predicts pairs of test labels--as opposed to learning multiple predictors for single test labels-- then coordinating the assignment of individual labels by propagating beliefs on a graph over the data. We argue that the approach is statistically well motivated, even for independent identically distributed (iid) data. In fact, we present experimental results that show improvements in classification accuracy over single-example classifiers, across a range of iid data sets and over a set of base classifiers. Like boosting, the technique increases representational capacity while controlling variance through a principled form of classifier combination."
ijcai,IJCAI (Artificial Intelligence),2005,A Probabilistic Model of Redundancy in Information Extraction,"Doug Downey, University of Washington|; et al.|Oren Etzioni, University of Washington|Stephen Soderland, University of Washington",https://www.semanticscholar.org/paper/310cd6a39b0539193561148cd9897b1953fa8b28,"Unsupervised Information Extraction (UIE) is the task of extracting knowledge from text without using hand-tagged training examples. A fundamental problem for both UIE and supervised IE is assessing the probability that extracted information is correct. In massive corpora such as the Web, the same extraction is found repeatedly in different documents. How does this redundancy impact the probability of correctness? 
 
This paper introduces a combinatorial ""balls-andurns"" model that computes the impact of sample size, redundancy, and corroboration from multiple distinct extraction rules on the probability that an extraction is correct. We describe methods for estimating the model's parameters in practice and demonstrate experimentally that for UIE the model's log likelihoods are 15 times better, on average, than those obtained by Pointwise Mutual Information (PMI) and the noisy-or model used in previous work. For supervised IE, the model's performance is comparable to that of Support Vector Machines, and Logistic Regression."
ijcai,IJCAI (Artificial Intelligence),2003,Thin Junction Tree Filters for Simultaneous Localization and Mapping,"Mark A. Paskin, University of California, Berkeley",https://www.semanticscholar.org/paper/7354cbc632175c1bff27532d0cb5231029f52b8e,"The Simultaneous Localization and Mapping problem is a fundamental problem in mobile robotics: while a robot navigates in an unknown environment, it must incrementally build a map of its surroundings and localize itself within that map. Traditional approaches to the problem are based upon Kalman filters, but suffer from complexity issues: first, the belief state grows quadratically in the size of the map; and second, the filtering operation can take time quadratic in the size of the map. I present a linear-space filter that maintains a tractable approximation of the belief state as a thin junction tree. The junction tree grows under measurement and motion updates and is periodically ""thinned"" to remain tractable. The time complexity of the filter operation is linear in the size of the map. I also present simple enhancements that permit constant-time approximate filtering."
ijcai,IJCAI (Artificial Intelligence),2003,Approximating Game-Theoretic Optimal Strategies for Full-scale Poker,"Darse Billings, University of Alberta|; et al.|Neil Burch, University of Alberta|Aaron Davidson, University of Alberta|Robert Holte, University of Alberta|Jonathan Schaeffer, University of Alberta|Terence Schauenberg, University of Alberta|Duane Szafron, University of Alberta",https://www.semanticscholar.org/paper/ce32b559b429f2549d38d67046409399a22bf91f,"The computation of the first complete approximations of game-theoretic optimal strategies for full-scale poker is addressed. Several abstraction techniques are combined to represent the game of 2-player Texas Hold'em, having size O(1018), using closely related models each having size O(1O7). Despite the reduction in size by a factor of 100 billion, the resulting models retain the key properties and structure of the real game. Linear programming solutions to the abstracted game are used to create substantially improved poker-playing programs, able to defeat strong human players and be competitive against world-class opponents."
ijcai,IJCAI (Artificial Intelligence),2001,Complexity Results for Structure-Based Causality,"Thomas Eiter & Thomas Lukasiewicz, TU Wien",https://www.semanticscholar.org/paper/dfcc94113515dc7108835e3afce9b89d7e4cb740,"Semantic Scholar extracted view of ""COMPLEXITY RESULTS FOR STRUCTURE-BASED CAUSALITY"" by Ur Informationssysteme"
ijcai,IJCAI (Artificial Intelligence),1999,A Distributed Case-Based Reasoning Application for Engineering Sales Support,"Ian Watson, University of Salford|Dan Gardingen, Western Air",https://www.semanticscholar.org/paper/b2c139a66b36c4daf3c9487d26b958643776e539,"This paper describes the implementation of a distributed case-based reasoning application that supports engineering sales staff. The application operates on the world wide web and uses the XML standard as a communications protocol between client and server side Java applets. The paper describes the distributed architecture of the application, the two case retrieval techniques used, its implementation, trial, roll-out and subsequent improvements to its architecture and retrieval techniques using introspective reasoning to improve retrieval efficiency. The benefits it has provided to the company are detailed."
ijcai,IJCAI (Artificial Intelligence),1999,Learning in Natural Language,"Dan Roth, University of Illinois at Urbana–Champaign",https://www.semanticscholar.org/paper/ca732052b6893b87c1c13c9f1f70809af39f55d1,"Statistics-based classifiers in natural language are developed typically by assuming a generative model for the data, estimating its parameters from training data and then using Bayes rule to obtain a classifier. For many problems the assumptions made by the generative models are evidently wrong, leaving open the question of why these approaches work. 
 
This paper presents a learning theory account of the major statistical approaches to learning in natural language. A class of Linear Statistical Queries (LSQ) hypotheses is defined and learning with it is shown to exhibit some robustness properties. Many statistical learners used in natural language, including naive Bayes, Markov Models and Maximum Entropy models are shown to be LSQ hypotheses, explaining the robustness of these predictors even when the underlying probabilistic assumptions do not hold. This coherent view of when and why learning approaches work in this context may help to develop better learning methods and an understanding of the role of learning in natural language inferences."
ijcai,IJCAI (Artificial Intelligence),1997,Translingual Information Retrieval: A Comparative Evaluation,"Jaime G. Carbonell, Carnegie Mellon University|; et al.|Yiming Yang, Carnegie Mellon University|Robert E. Frederking, Carnegie Mellon University|Ralf D. Brown, Carnegie Mellon University|Yibing Geng, Carnegie Mellon University|Danny Lee, Carnegie Mellon University",https://www.semanticscholar.org/paper/29480bf9360ad251e97e428015d8c4861d97eabc,Translingual information retrieval TIR con sists of providing a query in one language and searching document collections in one or more di erent languages This paper introduces new TIR methods and reports on comparative TIR experiments with these new methods and with previously reported ones in a realistic setting Methods fall into two categories query trans lation based and statistical IR approaches es tablishing translingual associations The re sults show that using bilingual corpora for au tomated extraction of term equivalences in con text outperforms other methods Translin gual versions of the Generalized Vector Space Model GVSM and Latent Semantic Indexing LSI perform relatively well as does translin gual pseudo relevance feedback PRF All showed relatively small performance loss be tween monolingual and translingual versions Query translation based on a general machine readable bilingual dictionary heretofore the most popular method did not match the per formance of other more sophisticated methods Also the previous very high LSI results in the literature were discon rmed by more realistic relevance based evaluations
ijcai,IJCAI (Artificial Intelligence),1997,Object Identification in a Bayesian Context,"Timothy Huang & Stuart Russell, University of California, Berkeley",https://www.semanticscholar.org/paper/a786ea2f9f5f1712ca1c8bebb839ccfaf6fe7795,"Object identification--the task of deciding that two observed objects are in fact one and the same object--is a fundamental requirement for any situated agent that reasons about individuals. Object identity, as represented by the equality operator between two terms in predicate calculus, is essentially a first-order concept. Raw sensory observations, on the other hand, are essentially propositional-- especially when formulated as evidence in standard probability theory. This paper describes patterns of reasoning that allow identity sentences to be grounded in sensory observations, thereby bridging the gap. We begin by defining a physical event space over which probabilities are defined. We then introduce an identity criterion, which selects those events that correspond to identity between observed objects. From this, we are able to compute the probability that any two objects are the same, given a stream of observations of many objects. We show that the appearance probability, which defines how an object can be expected to appear at subsequent observations given its current appearance, is a natural model for this type of reasoning. We apply the theory to the task of recognizing cars observed by cameras at widely separated sites in a freeway network, with new heuristics to handle the inevitable complexity of matching large numbers of objects and with online learning of appearance probability models. Despite extremely noisy observations, we are able to achieve high levels of performance."
ijcai,IJCAI (Artificial Intelligence),1997,Applications of the Situation Calculus to Formalizing Control and Strategic Information: The Prolo...,"Fangzhen Lin, The Hongkong University of Science and Technology",https://www.semanticscholar.org/paper/9b1d2fc5b3d97524ccdb3291de109e64c70cbe75,"We argue that the situation calculus is a natural formalism for representing and reasoning about control and strategic information. As a case study, in this paper we provide a situation calculus semantics for the Prolog cut operator, the central search control operator in Prolog. We show that our semantics is well-behaved when the programs are properly stastified. We also show that according to this semantics, the conventional implementation of the negationas-failure operator using cut is provably correct with respect to the stable model semantics."
sigmod,SIGMOD (Databases),2019,Interventional Fairness: Causal Database Repair for Algorithmic Fairness,"Babak Salimi, University of Washington|; et al.|Luke Rodriguez, University of Washington|Bill Howe, University of Washington|Dan Suciu, University of Washington",https://www.semanticscholar.org/paper/9977fa93c2451831edf30f98156ada39a19fcecb,"Fairness is increasingly recognized as a critical component of machine learning systems. However, it is the underlying data on which these systems are trained that often reflect discrimination, suggesting a database repair problem. Existing treatments of fairness rely on statistical correlations that can be fooled by statistical anomalies, such as Simpson's paradox. Proposals for causality-based definitions of fairness can correctly model some of these situations, but they require specification of the underlying causal models. In this paper, we formalize the situation as a database repair problem, proving sufficient conditions for fair classifiers in terms of admissible variables as opposed to a complete causal model. We show that these conditions correctly capture subtle fairness violations. We then use these conditions as the basis for database repair algorithms that provide provable fairness guarantees about classifiers trained on their training labels. We evaluate our algorithms on real data, demonstrating improvement over the state of the art on multiple fairness metrics proposed in the literature while retaining high utility."
sigmod,SIGMOD (Databases),2017,Parallelizing Sequential Graph Computations,"Wenfei Fan, University of Edinburgh|; et al.|Jingbo Xu, University of Edinburgh|Yinghui Wu, Washington State University|Wenyuan Yu, Beihang University|Jiaxin Jiang, Hong Kong Baptist University|Zeyu Zheng, Peking University|Bohan Zhang, Peking University|Yang Cao, University of Edinburgh|Chao Tian, University of Edinburgh",https://www.semanticscholar.org/paper/b75eaa6208939f5c5db3a4657e39f63956440f13,"This paper presents GRAPE, a parallel system for graph computations. GRAPE differs from prior systems in its ability to parallelize existing sequential graph algorithms as a whole. Underlying GRAPE are a simple programming model and a principled approach, based on partial evaluation and incremental computation. We show that sequential graph algorithms can be ""plugged into"" GRAPE with minor changes, and get parallelized. As long as the sequential algorithms are correct, their GRAPE parallelization guarantees to terminate with correct answers under a monotonic condition. Moreover, we show that algorithms in MapReduce, BSP and PRAM can be optimally simulated on GRAPE. In addition to the ease of programming, we experimentally verify that GRAPE achieves comparable performance to the state-of-the-art graph systems, using real-life and synthetic graphs."
sigmod,SIGMOD (Databases),2016,Wander Join: Online Aggregation via Random Walks,"Feifei Li, University of Utah|; et al.|Bin Wu, Hong Kong University of Science and Technology|Ke Yi, Hong Kong University of Science and Technology|Zhuoyue Zhao, Shanghai Jiao Tong University",https://www.semanticscholar.org/paper/e794e6e9881c673d9dd63d823e10118b209cae62,"Joins are expensive, and online aggregation over joins was proposed to mitigate the cost, which offers users a nice and flexible tradeoff between query efficiency and accuracy in a continuous, online fashion. However, the state-of-the-art approach, in both internal and external memory, is based on ripple join, which is still very expensive and even needs unrealistic assumptions (e.g., tuples in a table are stored in random order). This paper proposes a new approach, the wander join algorithm, to the online aggregation problem by performing random walks over the underlying join graph. We also design an optimizer that chooses the optimal plan for conducting the random walks without having to collect any statistics a priori. Compared with ripple join, wander join is particularly efficient for equality joins involving multiple tables, but also supports θ-joins. Selection predicates and group-by clauses can be handled as well. Extensive experiments using the TPC-H benchmark have demonstrated the superior performance of wander join over ripple join. In particular, we have integrated and tested wander join in the latest version of PostgreSQL, demonstrating its practicality in a full-fledged database system."
sigmod,SIGMOD (Databases),2015,"DBSCAN Revisited: Mis-Claim, Un-Fixability, and Approximation","Junhao Gan & Yufei Tao, The Chinese University of Hong Kong",https://www.semanticscholar.org/paper/5869b3d5607bff1a079aa24c8a241d656fe683b7,"DBSCAN is a popular method for clustering multi-dimensional objects. Just as notable as the method's vast success is the research community's quest for its efficient computation. The original KDD'96 paper claimed an algorithm with <i>O</i>(<i>n</i> log <i>n</i>) running time, where <i>n</i> is the number of objects. Unfortunately, this is a mis-claim; and that algorithm actually requires <i>O</i>(<i>n</i><sup>2</sup>) time. There has been a fix in 2D space, where a genuine <i>O</i>(<i>n</i> log <i>n</i>)-time algorithm has been found. Looking for a fix for dimensionality <i>d</i> ≥ 3 is currently an important open problem.
 In this paper, we prove that for <i>d</i> ≥ 3, the DBSCAN problem requires Ω(<i>n</i>4/<sup>3</sup>) time to solve, unless very significant breakthroughs---ones widely believed to be impossible---could be made in theoretical computer science. This (i) explains why the community's search for fixing the aforementioned mis-claim has been futile for <i>d</i> ≥ 3, and (ii) indicates (sadly) that <i>all</i> DBSCAN algorithms must be intolerably slow even on moderately large <i>n</i> in practice. Surprisingly, we show that the running time can be dramatically brought down to <i>O</i>(<i>n</i>) in expectation <i>regardless of the dimensionality d</i>, as soon as slight inaccuracy in the clustering results is permitted. We formalize our findings into the new notion of ρ-<i>approximate</i> DBSCAN, which we believe should replace DBSCAN on big data due to the latter's computational intractability."
sigmod,SIGMOD (Databases),2014,Materialization Optimizations for Feature Selection Workloads,"Ce Zhang, Stanford University|; et al.|Arun Kumar, University of Wisconsin–Madison|Christopher Ré, Stanford University",https://www.semanticscholar.org/paper/92e87ab1cb445b93b900007d7e6fdeee4eda07cc,"There is an arms race in the data management industry to support analytics, in which one critical step is feature selection, the process of selecting a feature set that will be used to build a statistical model. Analytics is one of the biggest topics in data management, and feature selection is widely regarded as the most critical step of analytics; thus, we argue that managing the feature selection process is a pressing data management challenge. We study this challenge by describing a feature-selection language and a supporting prototype system that builds on top of current industrial, R-integration layers. From our interactions with analysts, we learned that feature selection is an interactive, human-in-the-loop process, which means that feature selection workloads are rife with reuse opportunities. Thus, we study how to materialize portions of this computation using not only classical database materialization optimizations but also methods that have not previously been used in database optimization, including structural decomposition methods (like QR factorization) and warmstart. These new methods have no analog in traditional SQL systems, but they may be interesting for array and scientific database applications. On a diverse set of data sets and programs, we find that traditional database-style approaches that ignore these new opportunities are more than two orders of magnitude slower than an optimal plan in this new tradeoff space across multiple R-backends. Furthermore, we show that it is possible to build a simple cost-based optimizer to automatically select a near-optimal execution plan for feature selection."
sigmod,SIGMOD (Databases),2013,Massive Graph Triangulation,"Xiaocheng Hu, The Chinese University of Hong Kong|; et al.|Yufei Tao, The Chinese University of Hong Kong|Chin-Wan Chung, KAIST",https://www.semanticscholar.org/paper/fbeea48196d128288c0e314ba544497a7ac18771,"This paper studies I/O-efficient algorithms for settling the classic triangle listing problem, whose solution is a basic operator in dealing with many other graph problems. Specifically, given an undirected graph G, the objective of triangle listing is to find all the cliques involving 3 vertices in G. The problem has been well studied in internal memory, but remains an urgent difficult challenge when G does not fit in memory, rendering any algorithm to entail frequent I/O accesses. Although previous research has attempted to tackle the challenge, the state-of-the-art solutions rely on a set of crippling assumptions to guarantee good performance. Motivated by this, we develop a new algorithm that is provably I/O and CPU efficient at the same time, without making any assumption on the input G at all. The algorithm uses ideas drastically different from all the previous approaches, and outperformed the existing competitors by a factor over an order of magnitude in our extensive experimentation."
sigmod,SIGMOD (Databases),2012,High-Performance Complex Event Processing over XML Streams,"Barzan Mozafari, Massachusetts Institute of Technology|; et al.|Kai Zeng, University of California, Los Angeles|Carlo Zaniolo, University of California, Los Angeles",https://www.semanticscholar.org/paper/17f66d7a69bf20b29602d943069eedcd1c07abff,"Much research attention has been given to delivering high-performance systems that are capable of complex event processing (CEP) in a wide range of applications. However, many current CEP systems focus on processing efficiently data having a simple structure, and are otherwise limited in their ability to support efficiently complex continuous queries on structured or semi-structured information. However, XML streams represent a very popular form of data exchange, comprising large portions of social network and RSS feeds, financial records, configuration files, and similar applications requiring advanced CEP queries. In this paper, we present the XSeq language and system that support CEP on XML streams, via an extension of XPath that is both powerful and amenable to an efficient implementation. Specifically, the XSeq language extends XPath with natural operators to express sequential and Kleene-* patterns over XML streams, while remaining highly amenable to efficient implementation. XSeq is designed to take full advantage of recent advances in the field of automata on Visibly Pushdown Automata (VPA), where higher expressive power can be achieved without compromising efficiency (whereas the amenability to efficient implementation was not demonstrated in XPath extensions previously proposed).
 We illustrate XSeq's power for CEP applications through examples from different domains, and provide formal results on its expressiveness and complexity. Finally, we present several optimization techniques for XSeq queries. Our extensive experiments indicate that XSeq brings outstanding performance to CEP applications: two orders of magnitude improvement are obtained over the same queries executed in general-purpose XML engines."
sigmod,SIGMOD (Databases),2011,Entangled Queries: Enabling Declarative Data-Driven Coordination,"Nitin Gupta, Cornell University|; et al.|Lucja Kot, Cornell University|Sudip Roy, Cornell University|Gabriel Bender, Cornell University|Johannes Gehrke, Cornell University|Christoph Koch, École Polytechnique Fédérale de Lausanne",https://www.semanticscholar.org/paper/2f51f197f01d3e8fd4fc6d13618f6a1238ec5d15,"Many data-driven social and Web applications involve collaboration and coordination. The vision of declarative data-driven coordination (D3C), proposed in [9], is to support coordination in the spirit of data management: to make it data-centric and to specify it using convenient declarative languages. This paper introduces entangled queries, a language that extends SQL by constraints that allow for the coordinated choice of result tuples across queries originating from different users or applications.
 It is nontrivial to define a declarative coordination formalism without arriving at the general (NP-complete) Constraint Satisfaction Problem from AI. In this paper, we propose an efficiently enforcible syntactic safety condition that we argue is at the sweet spot where interesting declarative power meets applicability in large scale data management systems and applications.
 The key computational problem of D3C is to match entangled queries to achieve coordination. We present an efficient matching algorithm which statically analyzes query workloads and merges coordinating entangled queries into compound SQL queries. These can be sent to a standard database system and return only coordinated results. We present the overall architecture of an implemented system that contains our evaluation algorithm; we also evaluate the performance of the matching algorithm experimentally on realistic coordination workloads."
sigmod,SIGMOD (Databases),2010,FAST: fast architecture sensitive tree search on modern CPUs and GPUs,"Changkyu Kim, Intel|; et al.|Jatin Chhugani, Intel|Nadathur Satish, Intel|Eric Sedlar, Oracle Corporation|Anthony D. Nguyen, Intel|Tim Kaldewey, Oracle Corporation|Victor W. Lee, Intel|Scott A. Brandt, University of California, Santa Cruz|Pradeep Dubey, Intel",https://www.semanticscholar.org/paper/fc8aaceff4d907f025ec9b2d8f3a6980cb663090,"In-memory tree structured index search is a fundamental database operation. Modern processors provide tremendous computing power by integrating multiple cores, each with wide vector units. There has been much work to exploit modern processor architectures for database primitives like scan, sort, join and aggregation. However, unlike other primitives, tree search presents significant challenges due to irregular and unpredictable data accesses in tree traversal.
 In this paper, we present FAST, an extremely fast architecture sensitive layout of the index tree. FAST is a binary tree logically organized to optimize for architecture features like page size, cache line size, and SIMD width of the underlying hardware. FAST eliminates impact of memory latency, and exploits thread-level and datalevel parallelism on both CPUs and GPUs to achieve 50 million (CPU) and 85 million (GPU) queries per second, 5X (CPU) and 1.7X (GPU) faster than the best previously reported performance on the same architectures. FAST supports efficient bulk updates by rebuilding index trees in less than 0.1 seconds for datasets as large as 64Mkeys and naturally integrates compression techniques, overcoming the memory bandwidth bottleneck and achieving a 6X performance improvement over uncompressed index search for large keys on CPUs."
sigmod,SIGMOD (Databases),2009,Generating example data for dataflow programs,"Christopher Olston, Yahoo! Research|; et al.|Shubham Chopra, Yahoo! Research|Utkarsh Srivastava, Yahoo! Research",https://www.semanticscholar.org/paper/a3d069cba4e95b307070ec642e013347acefa891,"While developing data-centric programs, users often run (portions of) their programs over real data, to see how they behave and what the output looks like. Doing so makes it easier to formulate, understand and compose programs correctly, compared with examination of program logic alone. For large input data sets, these experimental runs can be time-consuming and inefficient. Unfortunately, sampling the input data does not always work well, because selective operations such as filter and join can lead to empty results over sampled inputs, and unless certain indexes are present there is no way to generate biased samples efficiently. Consequently new methods are needed for generating example input data for data-centric programs.
 We focus on an important category of data-centric programs, dataflow programs, which are best illustrated by displaying the series of intermediate data tables that occur between each pair of operations. We introduce and study the problem of generating example intermediate data for dataflow programs, in a manner that illustrates the semantics of the operators while keeping the example data small. We identify two major obstacles that impede naive approaches, namely (1) highly selective operators and (2) noninvertible operators, and offer techniques for dealing with these obstacles. Our techniques perform well on real dataflow programs used at Yahoo! for web analytics."
sigmod,SIGMOD (Databases),2008,Scalable Network Distance Browsing in Spatial Databases,"Hanan Samet, University of Maryland|; et al.|Jagan Sankaranarayanan, University of Maryland|Houman Alborzi, University of Maryland",https://www.semanticscholar.org/paper/8510872ba5fb52f2341178e691f26a69bd7487f4,"An algorithm is presented for finding the k nearest neighbors in a spatial network in a best-first manner using network distance. The algorithm is based on precomputing the shortest paths between all possible vertices in the network and then making use of an encoding that takes advantage of the fact that the shortest paths from vertex u to all of the remaining vertices can be decomposed into subsets based on the first edges on the shortest paths to them from u. Thus, in the worst case, the amount of work depends on the number of objects that are examined and the number of links on the shortest paths to them from q, rather than depending on the number of vertices in the network. The amount of storage required to keep track of the subsets is reduced by taking advantage of their spatial coherence which is captured by the aid of a shortest path quadtree. In particular, experiments on a number of large road networks as well as a theoretical analysis have shown that the storage has been reduced from O(N3) to O(N1.5) (i.e., by an order of magnitude equal to the square root). The precomputation of the shortest paths along the network essentially decouples the process of computing shortest paths along the network from that of finding the neighbors, and thereby also decouples the domain S of the query objects and that of the objects from which the neighbors are drawn from the domain V of the vertices of the spatial network. This means that as long as the spatial network is unchanged, the algorithm and underlying representation of the shortest paths in the spatial network can be used with different sets of objects."
sigmod,SIGMOD (Databases),2008,Serializable isolation for snapshot databases,"Michael J. Cahill, University of Sydney|; et al.|Uwe Röhm, University of Sydney|Alan D. Fekete, University of Sydney",https://www.semanticscholar.org/paper/e48e77dc76485228f7ea2229048f83aabdc3a582,"Many popular database management systems implement a multiversion concurrency control algorithm called snapshot isolation rather than providing full serializability based on locking. There are well-known anomalies permitted by snapshot isolation that can lead to violations of data consistency by interleaving transactions that would maintain consistency if run serially. Until now, the only way to prevent these anomalies was to modify the applications by introducing explicit locking or artificial update conflicts, following careful analysis of conflicts between all pairs of transactions.
 This article describes a modification to the concurrency control algorithm of a database management system that automatically detects and prevents snapshot isolation anomalies at runtime for arbitrary applications, thus providing serializable isolation. The new algorithm preserves the properties that make snapshot isolation attractive, including that readers do not block writers and vice versa. An implementation of the algorithm in a relational DBMS is described, along with a benchmark and performance study, showing that the throughput approaches that of snapshot isolation in most cases."
sigmod,SIGMOD (Databases),2007,Scalable Approximate Query Processing with the DBO Engine,"Christopher Jermaine, University of Florida|; et al.|Subramanian Arumugam, University of Florida|Abhijit Pol, University of Florida|Alin Dobra, University of Florida",https://www.semanticscholar.org/paper/b331ea79e42aafcc3217699320b54409e2d3babd,"This paper describes query processing in the DBO database system. Like other database systems designed for ad-hoc, analytic processing, DBO is able to compute the exact answer to queries over a large relational database in a scalable fashion. Unlike any other system designed for analytic processing, DBO can constantly maintain a guess as to the final answer to an aggregate query throughout execution, along with statistically meaningful bounds for the guess's accuracy. As DBO gathers more and more information, the guess gets more and more accurate, until it is 100% accurate as the query is completed. This allows users to stop the execution at any time that they are happy with the query accuracy, and encourages exploratory data analysis."
sigmod,SIGMOD (Databases),2007,Compiling mappings to bridge applications and databases,"Sergey Melnik, Microsoft Research|; et al.|Atul Adya, Microsoft|Philip A. Bernstein, Microsoft Research",https://www.semanticscholar.org/paper/90ef06729fe0c5ade5b10e5ee63f982f94edbda2,"Translating data and data access operations between applications and databases is a longstanding data management problem. We present a novel approach to this problem, in which the relationship between the application data and the persistent storage is specified using a declarative mapping, which is compiled into bidirectional views that drive the data transformation engine. Expressing the application model as a view on the database is used to answer queries, while viewing the database in terms of the application model allows us to leverage view maintenance algorithms for update translation. This approach has been implemented in a commercial product. It enables developers to interact with a relational database via a conceptual schema and an object oriented programming surface. We outline the implemented system and focus on the challenges of mapping compilation, which include rewriting queries under constraints and supporting non-relational constructs."
sigmod,SIGMOD (Databases),2006,To search or to crawl?: towards a query optimizer for text-centric tasks,"Panagiotis G. Ipeirotis, New York University|; et al.|Eugene Agichtein, Microsoft Research|Pranay Jain, Columbia University|Luis Gravano, Columbia University",https://www.semanticscholar.org/paper/2cf5c0b0da99c469da716f1658179d0ee374d085,"Text is ubiquitous and, not surprisingly, many important applications rely on textual data for a variety of tasks. As a notable example, information extraction applications derive structured relations from unstructured text; as another example, focused crawlers explore the web to locate pages about specific topics. Execution plans for text-centric tasks follow two general paradigms for processing a text database: either we can scan, or 'crawl,"" the text database or, alternatively, we can exploit search engine indexes and retrieve the documents of interest via carefully crafted queries constructed in task-specific ways. The choice between crawl- and query-based execution plans can have a substantial impact on both execution time and output ""completeness"" (e.g., in terms of recall). Nevertheless, this choice is typically ad-hoc and based on heuristics or plain intuition. In this paper, we present fundamental building blocks to make the choice of execution plans for text-centric tasks in an informed, cost-based way. Towards this goal, we show how to analyze query- and crawl-based plans in terms of both execution time and output completeness. We adapt results from random-graph theory and statistics to develop a rigorous cost model for the execution plans. Our cost model reflects the fact that the performance of the plans depends on fundamental task-specific properties of the underlying text databases. We identify these properties and present efficient techniques for estimating the associated cost-model parameters. Overall, our approach helps predict the most appropriate execution plans for a task, resulting in significant efficiency and output completeness benefits. We complement our results with a large-scale experimental evaluation for three important text-centric tasks and over multiple real-life data sets."
sigmod,SIGMOD (Databases),2004,Indexing spatio-temporal trajectories with Chebyshev polynomials,"Yuhan Cai & Raymond T. Ng, University of British Columbia",https://www.semanticscholar.org/paper/d108d181417d8e18d3dde91d1f645179e95130ce,"In this paper, we attempt to approximate and index a d- dimensional (d ≥ 1) spatio-temporal trajectory with a low order continuous polynomial. There are many possible ways to choose the polynomial, including (continuous)Fourier transforms, splines, non-linear regressino, etc. Some of these possiblities have indeed been studied beofre. We hypothesize that one of the best possibilities is the polynomial that minimizes the maximum deviation from the true value, which is called the minimax polynomial. Minimax approximation is particularly meaningful for indexing because in a branch-and-bound search (i.e., for finding nearest neighbours), the smaller the maximum deviation, the more pruning opportunities there exist. However, in general, among all the polynomials of the same degree, the optimal minimax polynomial is very hard to compute. However, it has been shown thta the Chebyshev approximation is almost identical to the optimal minimax polynomial, and is easy to compute [16]. Thus, in this paper, we explore how to use the Chebyshev polynomials as a basis for approximating and indexing d-dimenstional trajectories.The key analytic result of this paper is the Lower Bounding Lemma. that is, we show that the Euclidean distance between two d-dimensional trajectories is lower bounded by the weighted Euclidean distance between the two vectors of Chebyshev coefficients. this lemma is not trivial to show, and it ensures that indexing with Chebyshev cofficients aedmits no false negatives. To complement that analystic result, we conducted comprehensive experimental evaluation with real and generated 1-dimensional to 4-dimensional data sets. We compared the proposed schem with the Adaptive Piecewise Constant Approximation (APCA) scheme. Our preliminary results indicate that in all situations we tested, Chebyshev indexing dominates APCA in pruning power, I/O and CPU costs."
sigmod,SIGMOD (Databases),2003,Spreadsheets in RDBMS for OLAP,"Andrew Witkowski, Oracle Corporation|; et al.|Srikanth Bellakonda, Oracle Corporation|Tolga Bozkaya, Oracle Corporation|Gregory Dorman, Oracle Corporation|Nathan Folkert, Oracle Corporation|Abhinav Gupta, Oracle Corporation|Lei Shen, Oracle Corporation|Sankar Subramanian, Oracle Corporation",https://www.semanticscholar.org/paper/be278ebf93c1ad910152f3aba00b45d97780bb09,"One of the critical deficiencies of SQL is lack of support for n-dimensional array-based computations which are frequent in OLAP environments. Relational OLAP (ROLAP) applications have to emulate them using joins, recently introduced SQL Window Functions [18] and complex and inefficient CASE expressions. The designated place in SQL for specifying calculations is the SELECT clause, which is extremely limiting and forces the user to generate queries using nested views, subqueries and complex joins. Furthermore, SQL-query optimizer is pre-occupied with determining efficient join orders and choosing optimal access methods and largely disregards optimization of complex numerical formulas. Execution methods concentrated on efficient computation of a cube [11], [16] rather than on random access structures for inter-row calculations. This has created a gap that has been filled by spreadsheets and specialized MOLAP engines, which are good at formulas for mathematical modeling but lack the formalism of the relational model, are difficult to manage, and exhibit scalability problems. This paper presents SQL extensions involving array based calculations for complex modeling. In addition, we present optimizations, access structures and execution models for processing them efficiently."
sigmod,SIGMOD (Databases),2001,Locally adaptive dimensionality reduction for indexing large time series databases,"Eamonn Keogh, University of California, Irvine|; et al.|Kaushik Chakrabarti, University of California, Irvine|Michael Pazzani, University of California, Irvine|Sharad Mehrotra, University of California, Irvine",https://www.semanticscholar.org/paper/728211a7edfe536299670f34714de1b426d969dc,"Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data.. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this work we introduce a new dimensionality reduction technique which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower bounding, but very tight Euclidean distance approximation and show how they can support fast exact searching, and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority."
sigmod,SIGMOD (Databases),2000,XMill: an efficient compressor for XML data,"Hartmut Liefke, University of Pennsylvania|Dan Suciu, AT&T Laboratories",https://www.semanticscholar.org/paper/141e8528f0d3cdcf5dcbfe3ba5be5af494832c09,"We describe a tool for compressing XML data, with applications in data exchange and archiving, which usually achieves about twice the compression ratio of gzip at roughly the same speed. The compressor, called XMill, incorporates and combines existing compressors in order to apply them to heterogeneous XML data: it uses zlib, the library function for gzip, a collection of datatype specific compressors for simple data types, and, possibly, user defined compressors for application specific data types."
sigmod,SIGMOD (Databases),1999,DynaMat: a dynamic view management system for data warehouses,"Yannis Kotidis & Nick Roussopoulos, University of Maryland",https://www.semanticscholar.org/paper/d30053f74173c7bb1daa92ff3daa9e214f6c0819,"Pre-computation and materialization of views with aggregate functions is a common technique in Data Warehouses. Due to the complex structure of the warehouse and the different profiles of the users who submit queries, there is need for tools that will automate the selection and management of the materialized data. In this paper we present DynaMat, a system that dynamically materializes information at multiple levels of granularity in order to match the demand (workload) but also takes into account the maintenance restrictions for the warehouse, such as down time to update the views and space availability. DynaMat unifies the view selection and the view maintenance problems under a single framework using a novel “goodness” measure for the materialized views. DynaMat constantly monitors incoming queries and materializes the best set of views subject to the space constraints. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window. We compare DynaMat against a system that is given all queries in advance and the pre-computed optimal static view selection. The comparison is made based on a new metric, the Detailed Cost Savings Ratio introduced for quantifying the benefits of view materialization against incoming queries. These experiments show that DynaMat's dynamic view selection outperforms the optimal static view selection and thus, any sub-optimal static algorithm that has appeared in the literature."
sigmod,SIGMOD (Databases),1998,Efficient transparent application recovery in client-server information systems,"David Lomet & Gerhard Weikum, Microsoft Research",https://www.semanticscholar.org/paper/3bb2f341f17863c6185a3884394fa02ce1410c2c,"Database systems recover persistent data, providing high database availability. However, database applications, typically residing on client or “middle-tier” application-server machines, may lose work because of a server failure. This prevents the masking of server failures from the human user and substantially degrades application availability. This paper aims to enable high application availability with an integrated method for database server recovery and transparent application recovery in a client-server system. The approach, based on application message logging, is similar to earlier work on distributed system fault tolerance. However, we exploit advanced database logging and recovery techniques and request/reply messaging properties to significantly improve efficiency. Forced log I/Os, frequently required by other methods, are usually avoided. Restart time, for both failed server and failed client, is reduced by checkpointing and log truncation. Our method ensures that a server can recover independently of clients. A client may reduce logging overhead in return for dependency on server availability during client restart."
sigmod,SIGMOD (Databases),1998,Integrating association rule mining with relational database systems: alternatives and implications,"Sunita Sarawagi, IBM Research|; et al.|Shiby Thomas, University of Florida|Rakesh Agrawal, IBM Research",https://www.semanticscholar.org/paper/8716b5ab1185e6dd4ba8adf25753613d874d9cc9,"Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability."
sigmod,SIGMOD (Databases),1997,Fast parallel similarity search in multimedia databases,"Stefan Berchtold, Ludwig Maximilian University of Munich|; et al.|Christian Böml, Ludwig Maximilian University of Munich|Bernhard Braunmüller, Ludwig Maximilian University of Munich|Daniel A. Keim, Ludwig Maximilian University of Munich|Hans-Peter Kriegel, Ludwig Maximilian University of Munich",https://www.semanticscholar.org/paper/5b6f2a31b14b3f6bfc89384ae16a8f4652186df6,"Most similarity search techniques map the data objects into some high-dimensional feature space. The similarity search then corresponds to a nearest-neighbor search in the feature space which is computationally very intensive. In this paper, we present a new parallel method for fast nearest-neighbor search in high-dimensional feature spaces. The core problem of designing a parallel nearest-neighbor algorithm is to find an adequate distribution of the data onto the disks. Unfortunately, the known declustering methods to not perform well for high-dimensional nearest-neighbor search. In contrast, our method has been optimized based on the special properties of high-dimensional spaces and therefore provides a near-optimal distribution of the data items among the disks. The basic idea of our data declustering technique is to assign the buckets corresponding to different quadrants of the data space to different disks. We show that our technique - in contrast to other declustering methods - guarantees that all buckets corresponding to neighboring quadrants are assigned to different disks. We evaluate our method using large amounts of real data (up to 40 MBytes) and compare it with the best known data declustering method, the Hilbert curve. Our experiments show that our method provides an almost linear speed-up and a constant scale-up. Additionally, it outperforms the Hilbert approach by a factor of up to 5."
sigmod,SIGMOD (Databases),1996,Implementing data cubes efficiently,"Venky Harinarayan, Stanford University|; et al.|Anand Rajaraman, Stanford University|Jeffrey D. Ullman, Stanford University",https://www.semanticscholar.org/paper/f449bfc57afa2a0bd65de0173815b25ec4bf3046,"Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensional data cubes. Each cell of the data cube is a view consisting of an aggregation of interest, like total sales. The values of many of these cells are dependent on the values of other cells in the data cube. A common and powerful query optimization technique is to materialize some or all of these cells rather than compute them from raw data each time. Commercial systems differ mainly in their approach to materializing the data cube. In this paper, we investigate the issue of which cells (views) to materialize when it is too expensive to materialize all views. A lattice framework is used to express dependencies among views. We present greedy algorithms that work off this lattice and determine a good set of views to materialize. The greedy algorithm performs within a small constant factor of optimal under a variety of models. We then consider the most common case of the hypercube lattice and examine the choice of materialized views for hypercubes in detail, giving some good tradeoffs between the space used and the average time to answer a query."
sigir,SIGIR (Information Retrieval),2019,Variance Reduction in Gradient Exploration for Online Learning to Rank,"Huazheng Wang, University of Virginia|; et al.|Sunwoo Kim, University of Virginia|Eric McCord-Snook, University of Virginia|Qingyun Wu, University of Virginia|Hongning Wang, University of Virginia",https://www.semanticscholar.org/paper/538ce5a49afb614255aa669f45b60ec0fc89cce6,"Online Learning to Rank (OL2R) algorithms learn from implicit user feedback on the fly. The key to such algorithms is an unbiased estimate of gradients, which is often (trivially) achieved by uniformly sampling from the entire parameter space. Unfortunately, this leads to high-variance in gradient estimation, resulting in high regret during model updates, especially when the dimension of the parameter space is large. In this work, we aim at reducing the variance of gradient estimation in OL2R algorithms. We project the selected updating direction (i.e., the winning direction) into a space spanned by the feature vectors from examined documents under the current query (termed the ""document space"" for short), after an interleaved test. Our key insight is that the result of an interleaved test is solely governed by a user's relevance evaluation over the examined documents. Hence, the true gradient introduced by this test is only reflected in the constructed document space, and components of the proposed gradient which are orthogonal to the document space can be safely removed, for variance reduction purpose. We prove that this projected gradient is still an unbiased estimation of the true gradient, and show that this lower-variance gradient estimation results in significant regret reduction. Our proposed method is compatible with all existing OL2R algorithms which rank documents using a linear model. Extensive experimental comparisons with several state-of-the-art OL2R algorithms have confirmed the effectiveness of our proposed method in reducing the variance of gradient estimation and improving overall ranking performance."
sigir,SIGIR (Information Retrieval),2018,Should I Follow the Crowd? A Probabilistic Analysis of the Effectiveness of Popularity in Recommen...,"Rocío Cañamares & Pablo Castells, Autonomous University of Madrid",https://www.semanticscholar.org/paper/dc6f76cb0036c26560de4e8512042d2576f84b38,"The use of IR methodology in the evaluation of recommender systems has become common practice in recent years. IR metrics have been found however to be strongly biased towards rewarding algorithms that recommend popular items ""the same bias that state of the art recommendation algorithms display. Recent research has confirmed and measured such biases, and proposed methods to avoid them. The fundamental question remains open though whether popularity is really a bias we should avoid or not; whether it could be a useful and reliable signal in recommendation, or it may be unfairly rewarded by the experimental biases. We address this question at a formal level by identifying and modeling the conditions that can determine the answer, in terms of dependencies between key random variables, involving item rating, discovery and relevance. We find conditions that guarantee popularity to be effective or quite the opposite, and for the measured metric values to reflect a true effectiveness, or qualitatively deviate from it. We exemplify and confirm the theoretical findings with empirical results. We build a crowdsourced dataset devoid of the usual biases displayed by common publicly available data, in which we illustrate contradictions between the accuracy that would be measured in a common biased offline experimental setting, and the actual accuracy that can be measured with unbiased observations."
sigir,SIGIR (Information Retrieval),2017,BitFunnel: Revisiting Signatures for Search,"Bob Goodwin, Microsoft|; et al.|Michael Hopcroft, Microsoft|Dan Luu, Microsoft|Alex Clemmer, Heptio|Mihaela Curmei, Microsoft|Sameh Elnikety, Microsoft|Yuxiong He, Microsoft",https://www.semanticscholar.org/paper/2a695a9bec2a0f4e26fab3a631ea4ea51a2aab95,"Since the mid-90s there has been a widely-held belief that signature files are inferior to inverted files for text indexing. In recent years the Bing search engine has developed and deployed an index based on bit-sliced signatures. This index, known as BitFunnel, replaced an existing production system based on an inverted index. The driving factor behind the shift away from the inverted index was operational cost savings. This paper describes algorithmic innovations and changes in the cloud computing landscape that led us to reconsider and eventually field a technology that was once considered unusable. The BitFunnel algorithm directly addresses four fundamental limitations in bit-sliced block signatures. At the same time, our mapping of the algorithm onto a cluster offers opportunities to avoid other costs associated with signatures. We show these innovations yield a significant efficiency gain versus classic bit-sliced signatures and then compare BitFunnel with Partitioned Elias-Fano Indexes, MG4J, and Lucene."
sigir,SIGIR (Information Retrieval),2016,Understanding Information Need: An fMRI Study,"Yashar Moshfeghi, University of Glasgow|; et al.|Peter Triantafillou, University of Glasgow|Frank E. Pollick, University of Glasgow",https://www.semanticscholar.org/paper/1919e7c0f4c1b5252219f49f3c263db58768b1b2,"The raison d'etre of IR is to satisfy human information need. But, do we really understand information need? Despite advances in the past few decades in both the IR and relevant scientific communities, this question is largely unanswered. We do not really understand how an information need emerges and how it is physically manifested. Information need refers to a complex concept: at the very initial state of the phenomenon (i.e. at a visceral level), even the searcher may not be aware of its existence. This renders the measuring of this concept (using traditional behaviour studies) nearly impossible. In this paper, we investigate the connection between an information need and brain activity. Using functional Magnetic Resonance Imaging (fMRI), we measured the brain activity of twenty four participants while they performed a Question Answering (Q/A) Task, where the questions were carefully selected and developed from TREC-8 and TREC 2001 Q/A Track. The results of this experiment revealed a distributed network of brain regions commonly associated with activities related to information need and retrieval and differing brain activity in processing scenarios when participants knew the answer to a given question and when they did not and needed to search. We believe our study and conclusions constitute an important step in unravelling the nature of information need and therefore better satisfying it."
sigir,SIGIR (Information Retrieval),2015,QuickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees,"Claudio Lucchese, Istituto di Scienza e Tecnologie dell'Informazione|; et al.|Franco Maria Nardini, Istituto di Scienza e Tecnologie dell'Informazione|Salvatore Orlando, IUAV University of Venice|Raffaele Perego, Istituto di Scienza e Tecnologie dell'Informazione|Nicola Tonellotto, Istituto di Scienza e Tecnologie dell'Informazione|Rossano Venturini, Istituto di Scienza e Tecnologie dell'Informazione",https://www.semanticscholar.org/paper/2bb585c4b9d89b095e9938f7d1d3286e4ac2076f,"Learning-to-Rank models based on additive ensembles of regression trees have proven to be very effective for ranking query results returned by Web search engines, a scenario where quality and efficiency requirements are very demanding. Unfortunately, the computational cost of these ranking models is high. Thus, several works already proposed solutions aiming at improving the efficiency of the scoring process by dealing with features and peculiarities of modern CPUs and memory hierarchies. In this paper, we present QuickScorer, a new algorithm that adopts a novel bitvector representation of the tree-based ranking model, and performs an interleaved traversal of the ensemble by means of simple logical bitwise operations. The performance of the proposed algorithm are unprecedented, due to its cache-aware approach, both in terms of data layout and access patterns, and to a control flow that entails very low branch mis-prediction rates. The experiments on real Learning-to-Rank datasets show that QuickScorer is able to achieve speedups over the best state-of-the-art baseline ranging from 2x to 6.5x."
sigir,SIGIR (Information Retrieval),2014,Partitioned Elias-Fano Indexes,"Giuseppe Ottaviano, Istituto di Scienza e Tecnologie dell'Informazione|Rossano Venturini, University of Pisa",https://www.semanticscholar.org/paper/04ebe19c8edf2a7588005900e23b5fadbd6e357a,"The Elias-Fano representation of monotone sequences has been recently applied to the compression of inverted indexes, showing excellent query performance thanks to its efficient random access and search operations. While its space occupancy is competitive with some state-of-the-art methods such as gamma-delta-Golomb codes and PForDelta, it fails to exploit the local clustering that inverted lists usually exhibit, namely the presence of long subsequences of close identifiers. In this paper we describe a new representation based on partitioning the list into chunks and encoding both the chunks and their endpoints with Elias-Fano, hence forming a two-level data structure. This partitioning enables the encoding to better adapt to the local statistics of the chunk, thus exploiting clustering and improving compression. We present two partition strategies, respectively with fixed and variable-length chunks. For the latter case we introduce a linear-time optimization algorithm which identifies the minimum-space partition up to an arbitrarily small approximation factor. We show that our partitioned Elias-Fano indexes offer significantly better compression than plain Elias-Fano, while preserving their query time efficiency. Furthermore, compared with other state-of-the-art compressed encodings, our indexes exhibit the best compression ratio/query time trade-off."
sigir,SIGIR (Information Retrieval),2013,Beliefs and Biases in Web Search,"Ryen W. White, Microsoft Research",https://www.semanticscholar.org/paper/2f278d1dab0f6e3939c747a2fc2a4cecdfc912b9,"People's beliefs, and unconscious biases that arise from those beliefs, influence their judgment, decision making, and actions, as is commonly accepted among psychologists. Biases can be observed in information retrieval in situations where searchers seek or are presented with information that significantly deviates from the truth. There is little understanding of the impact of such biases in search. In this paper we study search-related biases via multiple probes: an exploratory retrospective survey, human labeling of the captions and results returned by a Web search engine, and a large-scale log analysis of search behavior on that engine. Targeting yes-no questions in the critical domain of health search, we show that Web searchers exhibit their own biases and are also subject to bias from the search engine. We clearly observe searchers favoring positive information over negative and more than expected given base rates based on consensus answers from physicians. We also show that search engines strongly favor a particular, usually positive, perspective, irrespective of the truth. Importantly, we show that these biases can be counterproductive and affect search outcomes; in our study, around half of the answers that searchers settled on were actually incorrect. Our findings have implications for search engine design, including the development of ranking algorithms that con-sider the desire to satisfy searchers (by validating their beliefs) and providing accurate answers and properly considering base rates. Incorporating likelihood information into search is particularly important for consequential tasks, such as those with a medical focus."
sigir,SIGIR (Information Retrieval),2012,Time-Based Calibration of Effectiveness Measures,"Mark Smucker & Charles Clarke, University of Waterloo",https://www.semanticscholar.org/paper/aeda29c21d1f6eb89b1bed6f85e82aadfcc15d76,"Many current effectiveness measures incorporate simplifying assumptions about user behavior. These assumptions prevent the measures from reflecting aspects of the search process that directly impact the quality of retrieval results as experienced by the user. In particular, these measures implicitly model users as working down a list of retrieval results, spending equal time assessing each document. In reality, even a careful user, intending to identify as much relevant material as possible, must spend longer on some documents than on others. Aspects such as document length, duplicates and summaries all influence the time required. In this paper, we introduce a time-biased gain measure, which explicitly accommodates such aspects of the search process. By conducting an appropriate user study, we calibrate and validate the measure against the TREC 2005 Robust Track test collection. We examine properties of the measure, contrasting it to traditional effectiveness measures, and exploring its extension to other aspects and environments. As its primary benefit, the measure allows us to evaluate system performance in human terms, while maintaining the simplicity and repeatability of system-oriented tests. Overall, we aim to achieve a clearer connection between user-oriented studies and system-oriented tests, allowing us to better transfer insights and outcomes from one to the other."
sigir,SIGIR (Information Retrieval),2011,Find It If You Can: A Game for Modeling Different Types of Web Search Success Using Interaction Data,"Mikhail Ageev, Moscow State University|; et al.|Qi Guo, Emory University|Dmitry Lagun, Emory University|Eugene Agichtein, Emory University",https://www.semanticscholar.org/paper/d2f20cd0d58e60c8bfdebf424d354f24e98aad0c,"A better understanding of strategies and behavior of successful searchers is crucial for improving the experience of all searchers. However, research of search behavior has been struggling with the tension between the relatively small-scale, but controlled lab studies, and the large-scale log-based studies where the searcher intent and many other important factors have to be inferred. We present our solution for performing controlled, yet realistic, scalable, and reproducible studies of searcher behavior. We focus on difficult informational tasks, which tend to frustrate many users of the current web search technology. First, we propose a principled formalization of different types of ""success"" for informational search, which encapsulate and sharpen previously proposed models. Second, we present a scalable game-like infrastructure for crowdsourcing search behavior studies, specifically targeted towards capturing and evaluating successful search strategies on informational tasks with known intent. Third, we report our analysis of search success using these data, which confirm and extends previous findings. Finally, we demonstrate that our model can predict search success more effectively than the existing state-of-the-art methods, on both our data and on a different set of log data collected from regular search engine sessions. Together, our search success models, the data collection infrastructure, and the associated behavior analysis techniques, significantly advance the study of success in web search."
sigir,SIGIR (Information Retrieval),2010,Assessing the Scenic Route: Measuring the Value of Search Trails in Web Logs,"Ryen W. White, Microsoft Research|Jeff Huang, University of Washington",https://www.semanticscholar.org/paper/a97f142557e3645c572a6d144b9db90d97546806,"Search trails mined from browser or toolbar logs comprise queries and the post-query pages that users visit. Implicit endorsements from many trails can be useful for search result ranking, where the presence of a page on a trail increases its query relevance. Follow-ing a search trail requires user effort, yet little is known about the benefit that users obtain from this activity versus, say, sticking with the clicked search result or jumping directly to the destination page at the end of the trail. In this paper, we present a log-based study estimating the user value of trail following. We compare the relevance, topic coverage, topic diversity, novelty, and utility of full trails over that provided by sub-trails, trail origins (landing pages), and trail destinations (pages where trails end). Our findings demonstrate significant value to users in following trails, especially for certain query types. The findings have implications for the design of search systems, including trail recommendation systems that display trails on search result pages."
sigir,SIGIR (Information Retrieval),2009,Sources of evidence for vertical selection,"Jaime Arguello, Carnegie Mellon University|; et al.|Fernando Diaz, Yahoo! Research|Jamie Callan, Carnegie Mellon University|Jean-Francois Crespo, Yahoo! Research",https://www.semanticscholar.org/paper/47cdb987ec20ed2c1cdfca75ce2edf982ecfc5c4,"Web search providers often include search services for domain-specific subcollections, called verticals, such as news, images, videos, job postings, company summaries, and artist profiles. We address the problem of vertical selection, predicting relevant verticals (if any) for queries issued to the search engine's main web search page. In contrast to prior query classification and resource selection tasks, vertical selection is associated with unique resources that can inform the classification decision. We focus on three sources of evidence: (1) the query string, from which features are derived independent of external resources, (2) logs of queries previously issued directly to the vertical, and (3) corpora representative of vertical content. We focus on 18 different verticals, which differ in terms of semantics, media type, size, and level of query traffic. We compare our method to prior work in federated search and retrieval effectiveness prediction. An in-depth error analysis reveals unique challenges across different verticals and provides insight into vertical selection for future work."
sigir,SIGIR (Information Retrieval),2008,Algorithmic Mediation for Collaborative Exploratory Search,"Jeremy Pickens, FX Palo Alto Lab|; et al.|Gene Golovchinsky, FX Palo Alto Lab|Chirag Shah, University of North Carolina at Chapel Hill|Pernilla Qvarfordt, FX Palo Alto Lab|Maribeth Back, FX Palo Alto Lab",https://www.semanticscholar.org/paper/3bb44af2251b89c83a922778be5ba50cc205cc8c,"We describe a new approach to information retrieval: algorithmic mediation for intentional, synchronous collaborative exploratory search. Using our system, two or more users with a common information need search together, simultaneously. The collaborative system provides tools, user interfaces and, most importantly, algorithmically-mediated retrieval to focus, enhance and augment the team's search and communication activities. Collaborative search outperformed post hoc merging of similarly instrumented single user runs. Algorithmic mediation improved both collaborative search (allowing a team of searchers to find relevant information more efficiently and effectively), and exploratory search (allowing the searchers to find relevant information that cannot be found while working individually)."
sigir,SIGIR (Information Retrieval),2007,Studying the Use of Popular Destinations to Enhance Web Search Interaction,"Ryen W. White, Microsoft Research|; et al.|Mikhail Bilenko, Microsoft Research|Silviu Cucerzan, Microsoft Research",https://www.semanticscholar.org/paper/23d732e8d81457fa1b2149ebef32a62fd8c7b36d,"We present a novel Web search interaction feature which, for a given query, provides links to websites frequently visited by other users with similar information needs. These popular destinations complement traditional search results, allowing direct navigation to authoritative resources for the query topic. Destinations are identified using the history of search and browsing behavior of many users over an extended time period, whose collective behavior provides a basis for computing source authority. We describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries, as well as with traditional, unaided Web search. Results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks, with best performance obtained from mining past user behavior at query-level granularity."
sigir,SIGIR (Information Retrieval),2006,Minimal Test Collections for Retrieval Evaluation,"Ben Carterette, University of Massachusetts Amherst|; et al.|James Allan, University of Massachusetts Amherst|Ramesh Sitaraman, University of Massachusetts Amherst",https://www.semanticscholar.org/paper/34ddfe4d4fbf4afa586220a21a921478c8dfab35,"Accurate estimation of information retrieval evaluation metrics such as average precision require large sets of relevance judgments. Building sets large enough for evaluation of real-world implementations is at best inefficient, at worst infeasible. In this work we link evaluation with test collection construction to gain an understanding of the minimal judging effort that must be done to have high confidence in the outcome of an evaluation. A new way of looking at average precision leads to a natural algorithm for selecting documents to judge and allows us to estimate the degree of confidence by defining a distribution over possible document judgments. A study with annotators shows that this method can be used by a small group of researchers to rank a set of systems in under three hours with 95% confidence. Information retrieval metrics such as average precision require large sets of relevance judgments to be accurately estimated. Building these sets is infeasible and often inefficient for many real-world retrieval implementations. We present a new way of looking at average precision that allows us to estimate the confidence in an evaluation based on the size of the test collection. We use this to build an algorithm for selecting the best documents to judge to have maximum confidence in an evaluation with a minimal number of relevance judgments. A study with annotators shows how the algorithm can be used by a small group of researchers to quickly rank a set of systems with 95% confidence."
sigir,SIGIR (Information Retrieval),2005,Learning to estimate query difficulty: including applications to missing content detection and dis...,"Elad Yom-Tov, IBM Research|; et al.|Shai Fine, IBM Research|David Carmel, IBM Research|Adam Darlow, IBM Research",https://www.semanticscholar.org/paper/a74f5c9922c1327b94c70149944e3340263de961,"In this article we present novel learning methods for estimating the quality of results returned by a search engine in response to a query. Estimation is based on the agreement between the top results of the full query and the top results of its sub-queries. We demonstrate the usefulness of quality estimation for several applications, among them improvement of retrieval, detecting queries for which no relevant content exists in the document collection, and distributed information retrieval. Experiments on TREC data demonstrate the robustness and the effectiveness of our learning algorithms."
sigir,SIGIR (Information Retrieval),2004,A Formal Study of Information Retrieval Heuristics,"Hui Fang, University of Illinois at Urbana–Champaign|; et al.|Tao Tao, University of Illinois at Urbana–Champaign|ChengXiang Zhai, University of Illinois at Urbana–Champaign",https://www.semanticscholar.org/paper/c5445155baf89ad8d17a04fc82d0f9626558d5e6,"Empirical studies of information retrieval methods show that good retrieval performance is closely related to the use of various retrieval heuristics, such as TF-IDF weighting. One basic research question is thus what exactly are these ""necessary"" heuristics that seem to cause good retrieval performance. In this paper, we present a formal study of retrieval heuristics. We formally define a set of basic desirable constraints that any reasonable retrieval function should satisfy, and check these constraints on a variety of representative retrieval functions. We find that none of these retrieval functions satisfies all the constraints unconditionally. Empirical results show that when a constraint is not satisfied, it often indicates non-optimality of the method, and when a constraint is satisfied only for a certain range of parameter values, its performance tends to be poor when the parameter is out of the range. In general, we find that the empirical performance of a retrieval formula is tightly related to how well it satisfies these constraints. Thus the proposed constraints provide a good explanation of many empirical observations and make it possible to evaluate any existing or new retrieval formula analytically."
sigir,SIGIR (Information Retrieval),2003,Re-examining the potential effectiveness of interactive query expansion,"Ian Ruthven, University of Strathclyde",https://www.semanticscholar.org/paper/c5b9096e0945e779dd092a58937646d0563be0b8,"Much attention has been paid to the relative effectiveness of interactive query expansion versus automatic query expansion. Although interactive query expansion has the potential to be an effective means of improving a search, in this paper we show that, on average, human searchers are less likely than systems to make good expansion decisions. To enable good expansion decisions, searchers must have adequate instructions on how to use interactive query expansion functionalities. We show that simple instructions on using interactive query expansion do not necessarily help searchers make good expansion decisions and discuss difficulties found in making query expansion decisions."
sigir,SIGIR (Information Retrieval),2002,Novelty and redundancy detection in adaptive filtering,"Yi Zhang, Carnegie Mellon University|; et al.|Jamie Callan, Carnegie Mellon University|Thomas Minka, Carnegie Mellon University",https://www.semanticscholar.org/paper/2f7f81529c67e9d98722f4ee17fe3808f0752b5e,This paper addresses the problem of extending an adaptive information filtering system to make decisions about the novelty and redundancy of relevant documents. It argues that relevance and redundance should each be modelled explicitly and separately. A set of five redundancy measures are proposed and evaluated in experiments with and without redundancy thresholds. The experimental results demonstrate that the cosine similarity metric and a redundancy measure based on a mixture of language models are both effective for identifying redundant documents.
sigir,SIGIR (Information Retrieval),2001,Temporal summaries of new topics,"James Allan, University of Massachusetts Amherst|; et al.|Rahul Gupta, University of Massachusetts Amherst|Vikas Khandelwal, University of Massachusetts Amherst",https://www.semanticscholar.org/paper/b3365e64e4ce52823e25eea0fa16096d6cf383f0,"We discuss technology to help a person monitor changes in news coverage over time. We define temporal summaries of news stories as extracting a single sentence from each event within a news topic, where the stories are presented one at a time and sentences from a story must be ranked before the next story can be considered. We explain a method for evaluation, and describe an evaluation corpus that we have built. We also propose several methods for constructing temporal summaries and evaluate their effectiveness in comparison to degenerate cases. We show that simple approaches are effective, but that the problem is far from solved."
sigir,SIGIR (Information Retrieval),2000,IR evaluation methods for retrieving highly relevant documents,"Kalervo Järvelin & Jaana Kekäläinen, University of Tampere",https://www.semanticscholar.org/paper/93b9d22e6b1f3fc05feba3c5c3922a23dce09ea9,"This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in modem large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In- Query I) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods."
sigir,SIGIR (Information Retrieval),1998,A theory of term weighting based on exploratory data analysis,"Warren R. Greiff, University of Massachusetts Amherst",https://www.semanticscholar.org/paper/ee57ce8ffd5988dbbd18579afeb73eb6a3d16af6,"Techniques of exploratory data analysis are used to study the weight of evidence that the occurrence of a query term provides in support of the hypothesis that a document is relevant to an information need. In particular, the relationship between the document frequency and the weight of evidence is investigated. A correlation between document frequency normalized by collection size and the mutual information between relevance and term occurrence is uncovered. This correlation is found to be robust across a variety of query sets and document collections. Based on this relationship, a theoretical explanation of the efficacy of inverse document frequency for term weighting is developed which differs in both style and content from theories previously put forth. The theory predicts that a “flattening” of idf at both low and high frequency should result in improved retrieval performance. This altered idf formulation is tested on all TREC query sets. Retrieval results corroborate the prediction of improved retrieval performance. In conclusion, we argue that exploratory data analysis can be a valuable tool for research whose goal is the development of an explanatory theory of information retrieval."
sigir,SIGIR (Information Retrieval),1997,"Feature selection, perceptron learning, and a usability case study for text categorization","Hwee Tou Ng, Defence Science Organisation|; et al.|Wei Boon Goh, Ministry of Defence|Kok Leong Low, Ministry of Defence",https://www.semanticscholar.org/paper/0c97e8fcd80d9a3779826f2930724c9d789faa05,"In this paper, we describe an automated learning approach to text categorization based on perception learning and a new feature selection metric, called correlation coefficient. Our approach has been teated on the standard Reuters text categorization collection. Empirical results indicate that our approach outperforms the best published results on this % uters collection. In particular, our new feature selection method yields comiderable improvement. We also investigate the usability of our automated hxu-n~ approach by actually developing a system that categorizes texts into a treeof categories. We compare tbe accuracy of our learning approach to a rrddmsed, expert system ap preach that uses a text categorization shell built by Cams gie Group. Although our automated learning approach still gives a lower accuracy, by appropriately inmrporating a set of manually chosen worda to use as f~ures, the combined, semi-automated approach yields accuracy close to the * baaed approach."
sigir,SIGIR (Information Retrieval),1996,Retrieving spoken documents by combining multiple index sources,"Gareth Jones, University of Cambridge|; et al.|Jonathan Foote, University of Cambridge|Karen Spärck Jones, University of Cambridge|Steve Young, University of Cambridge",https://www.semanticscholar.org/paper/85fd61753b8f3574d79a4454349594c33e1624af,"This paper presents domain-independent methods of spoken document retrieval. Both a continuous-speech large vocabulary recognition system, and a phone-lattice word spotter, are used to locate index units within an experimental corpus of voice messages. Possible index terms are nearly unconstrained; terms not in a 20,000 word recognition system vocabulary can be identified by the word spotter at search time. Though either system alone can yield respectable retrieval performance, the two methods are complementary and work best in combination. Different ways of combining them are investigated, and it is shown that the best of these can increase retrieval average precision for a speakerindependent retrieval system to 85% of that achieved for full-text transcriptions of the test documents."
